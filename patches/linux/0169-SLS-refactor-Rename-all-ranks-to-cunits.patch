From 22840bb50cd839165cb2ececc064d8ea285712c0 Mon Sep 17 00:00:00 2001
From: Savelii Motov <s.motov@samsung.com>
Date: Thu, 24 Aug 2023 15:07:13 +0000
Subject: [PATCH 169/225] [SLS][refactor] Rename all ranks to cunits

Related to: MCS23-1373

Signed-off-by: Savelii Motov <s.motov@samsung.com>
---
 drivers/pnm/sls_resource/Makefile          |   2 +-
 drivers/pnm/sls_resource/allocator.c       | 110 +++----
 drivers/pnm/sls_resource/allocator.h       |   4 +-
 drivers/pnm/sls_resource/cunit_scheduler.c | 359 +++++++++++++++++++++
 drivers/pnm/sls_resource/cunit_scheduler.h |  30 ++
 drivers/pnm/sls_resource/process_manager.c | 145 +++++----
 drivers/pnm/sls_resource/process_manager.h |   6 +-
 drivers/pnm/sls_resource/rank_scheduler.c  | 357 --------------------
 drivers/pnm/sls_resource/rank_scheduler.h  |  30 --
 drivers/pnm/sls_resource/sls.c             |  26 +-
 drivers/pnm/sls_resource/sysfs.c           | 211 ++++++------
 include/uapi/linux/sls_resources.h         |  84 ++---
 12 files changed, 687 insertions(+), 677 deletions(-)
 create mode 100644 drivers/pnm/sls_resource/cunit_scheduler.c
 create mode 100644 drivers/pnm/sls_resource/cunit_scheduler.h
 delete mode 100644 drivers/pnm/sls_resource/rank_scheduler.c
 delete mode 100644 drivers/pnm/sls_resource/rank_scheduler.h

diff --git a/drivers/pnm/sls_resource/Makefile b/drivers/pnm/sls_resource/Makefile
index 1c67d30f9..3defa63a3 100644
--- a/drivers/pnm/sls_resource/Makefile
+++ b/drivers/pnm/sls_resource/Makefile
@@ -7,7 +7,7 @@ topo-y := topo/export.o
 topo-y += topo/params.o
 
 sls_resource-y := allocator.o
-sls_resource-y += rank_scheduler.o
+sls_resource-y += cunit_scheduler.o
 sls_resource-y += mem_info.o
 sls_resource-y += process_manager.o
 sls_resource-y += sls.o
diff --git a/drivers/pnm/sls_resource/allocator.c b/drivers/pnm/sls_resource/allocator.c
index feb0dd295..153f5ec8d 100644
--- a/drivers/pnm/sls_resource/allocator.c
+++ b/drivers/pnm/sls_resource/allocator.c
@@ -40,32 +40,33 @@ void unlock_sls_allocator(void)
 	mutex_unlock(&memory_mutex);
 }
 
-static int init_rank_pool(struct allocator *alloc, uint8_t rank, uint64_t size)
+static int init_cunit_pool(struct allocator *alloc, uint8_t cunit,
+			   uint64_t size)
 {
 	int err_code;
 	struct gen_pool *pool =
 		gen_pool_create(ilog2(alloc->gran), NUMA_NO_NODE);
 
 	if (unlikely(!pool)) {
-		SLS_ERR("gen_pool_create failed for rank pool[%hhu], granularity = [%llu]\n",
-			rank, alloc->gran);
+		SLS_ERR("gen_pool_create failed for cunit pool[%hhu], granularity = [%llu]\n",
+			cunit, alloc->gran);
 		err_code = -ENOMEM;
-		goto init_rank_pool_out;
+		goto init_cunit_pool_out;
 	}
 
 	err_code = gen_pool_add(pool, alloc->gran, size, NUMA_NO_NODE);
 	if (unlikely(err_code < 0)) {
-		SLS_ERR("Failed to init memory rank pool[%hhu], size = [%llu] with error [%d]\n",
-			rank, size, err_code);
+		SLS_ERR("Failed to init memory cunit pool[%hhu], size = [%llu] with error [%d]\n",
+			cunit, size, err_code);
 		gen_pool_destroy(pool);
-		goto init_rank_pool_out;
+		goto init_cunit_pool_out;
 	}
 
-	SLS_INF("Memory rank pool[%hhu] initialized: granularity = [%llu], size = [%llu]\n",
-		rank, alloc->gran, size);
-	alloc->pools[rank] = pool;
+	SLS_INF("Memory cunit pool[%hhu] initialized: granularity = [%llu], size = [%llu]\n",
+		cunit, alloc->gran, size);
+	alloc->pools[cunit] = pool;
 
-init_rank_pool_out:
+init_cunit_pool_out:
 	return err_code;
 }
 
@@ -79,19 +80,19 @@ static void mem_pool_mark_zero_chunk_size(struct gen_pool *pool,
 
 static void cleanup_pools(struct allocator *alloc)
 {
-	uint8_t rank;
+	uint8_t cunit;
 	struct gen_pool *pool;
 
-	for (rank = 0; rank < alloc->nr_pools; ++rank) {
-		pool = alloc->pools[rank];
+	for (cunit = 0; cunit < alloc->nr_pools; ++cunit) {
+		pool = alloc->pools[cunit];
 		if (unlikely(!pool)) {
-			SLS_WRN("Trying to cleanup memory rank pool[%hhu] that was not created\n",
-				rank);
+			SLS_WRN("Trying to cleanup memory cunit pool[%hhu] that was not created\n",
+				cunit);
 			continue;
 		}
 		if (unlikely(gen_pool_avail(pool) != gen_pool_size(pool))) {
 			SLS_ERR("Pool[%hhu]: non-deallocated objects, size: %zu, avail: %zu\n",
-				rank, gen_pool_size(pool),
+				cunit, gen_pool_size(pool),
 				gen_pool_avail(pool));
 			/*
 			 * To destroy memory pool gen_pool_destroy checks
@@ -103,12 +104,12 @@ static void cleanup_pools(struct allocator *alloc)
 				pool, mem_pool_mark_zero_chunk_size, NULL);
 		}
 		gen_pool_destroy(pool);
-		SLS_INF("Memory rank pool[%hhu] is destroyed\n", rank);
+		SLS_INF("Memory cunit pool[%hhu] is destroyed\n", cunit);
 	}
 	kfree(alloc->pools);
 }
 
-static inline uint64_t get_rank_size(struct allocator *alloc, uint8_t cunit)
+static inline uint64_t get_cunit_size(struct allocator *alloc, uint8_t cunit)
 {
 	const uint64_t nr_cunits = alloc->mem_cunit_info->nr_cunits;
 	const uint64_t nr_regions_per_cunit =
@@ -127,8 +128,8 @@ static inline uint64_t get_rank_size(struct allocator *alloc, uint8_t cunit)
 
 static int init_pools(struct allocator *alloc)
 {
-	uint8_t rank;
-	uint64_t rank_size;
+	uint8_t cunit;
+	uint64_t cunit_size;
 	int err_code;
 
 	alloc->pools =
@@ -136,7 +137,7 @@ static int init_pools(struct allocator *alloc)
 	if (!alloc->pools)
 		return -ENOMEM;
 
-	for (rank = 0; rank < alloc->nr_pools; ++rank) {
+	for (cunit = 0; cunit < alloc->nr_pools; ++cunit) {
 		/*
 		 * reserve PAGE_SIZE size from start to eliminate allocator
 		 * ambiguity, technically we could back off by just 1 byte,
@@ -145,8 +146,8 @@ static int init_pools(struct allocator *alloc)
 		 * the real allocation, and we'll not be able to distinguish
 		 * the result from the allocator fail.
 		 */
-		rank_size = get_rank_size(alloc, rank) - alloc->gran;
-		err_code = init_rank_pool(alloc, rank, rank_size);
+		cunit_size = get_cunit_size(alloc, cunit) - alloc->gran;
+		err_code = init_cunit_pool(alloc, cunit, cunit_size);
 		if (unlikely(err_code)) {
 			cleanup_pools(alloc);
 			return err_code;
@@ -217,29 +218,30 @@ void cleanup_sls_allocator(void)
 	mutex_destroy(&memory_mutex);
 }
 
-static uint8_t select_optimal_rank(struct allocator *alloc)
+static uint8_t select_optimal_cunit(struct allocator *alloc)
 {
-	uint8_t rank;
-	uint8_t optimal_rank = 0;
+	uint8_t cunit;
+	uint8_t optimal_cunit = 0;
 	size_t max_free_space = gen_pool_avail(alloc->pools[0]);
 
-	for (rank = 1; rank < alloc->nr_pools; ++rank) {
-		size_t rank_free_space = gen_pool_avail(alloc->pools[rank]);
+	for (cunit = 1; cunit < alloc->nr_pools; ++cunit) {
+		size_t cunit_free_space = gen_pool_avail(alloc->pools[cunit]);
 
-		if (max_free_space < rank_free_space) {
-			max_free_space = rank_free_space;
-			optimal_rank = rank;
+		if (max_free_space < cunit_free_space) {
+			max_free_space = cunit_free_space;
+			optimal_cunit = cunit;
 		}
 	}
-	return optimal_rank;
+	return optimal_cunit;
 }
 
 // Call only when holding allocator lock
 static int allocate_memory_unsafe(struct allocator *alloc,
 				  struct sls_memory_alloc_request *req)
 {
-	if (req->rank != SLS_ALLOC_ANY_RANK && req->rank >= alloc->nr_pools) {
-		SLS_ERR("Rank %u doesn't exist.\n", req->rank);
+	if (req->cunit != SLS_ALLOC_ANY_CUNIT &&
+	    req->cunit >= alloc->nr_pools) {
+		SLS_ERR("cunit %u doesn't exist.\n", req->cunit);
 		return -EINVAL;
 	}
 
@@ -248,18 +250,18 @@ static int allocate_memory_unsafe(struct allocator *alloc,
 		return -EINVAL;
 	}
 
-	if (req->rank == SLS_ALLOC_ANY_RANK)
-		req->rank = select_optimal_rank(alloc);
+	if (req->cunit == SLS_ALLOC_ANY_CUNIT)
+		req->cunit = select_optimal_cunit(alloc);
 
-	req->rank_offset = gen_pool_alloc(alloc->pools[req->rank], req->size);
+	req->cunit_offset = gen_pool_alloc(alloc->pools[req->cunit], req->size);
 
-	if (unlikely(req->rank_offset == 0)) {
+	if (unlikely(req->cunit_offset == 0)) {
 		SLS_ERR("No free memory for object of size = [%llu] at pool[%hhu]\n",
-			req->size, req->rank);
+			req->size, req->cunit);
 		return -ENOMEM;
 	}
 
-	req->rank_offset -= alloc->gran;
+	req->cunit_offset -= alloc->gran;
 
 	return 0;
 }
@@ -301,8 +303,8 @@ static int allocate_memory_ioctl(struct file *filp, struct allocator *alloc,
 		err = sls_proc_register_alloc(filp, kreq);
 
 		if (!err) {
-			SLS_DBG("Allocated obj: pool[%u], rank_offset = [0x%llx], size = [%llu]\n",
-				kreq.rank, kreq.rank_offset, kreq.size);
+			SLS_DBG("Allocated obj: pool[%u], cunit_offset = [0x%llx], size = [%llu]\n",
+				kreq.cunit, kreq.cunit_offset, kreq.size);
 		}
 	}
 	unlock_sls_allocator();
@@ -313,10 +315,10 @@ static int allocate_memory_ioctl(struct file *filp, struct allocator *alloc,
 // Call only when holding allocator lock
 int deallocate_memory_unsafe(struct sls_memory_alloc_request req)
 {
-	req.rank_offset += allocator.gran;
+	req.cunit_offset += allocator.gran;
 
-	if (req.rank >= allocator.nr_pools) {
-		SLS_ERR("Rank %u doesn't exist.\n", req.rank);
+	if (req.cunit >= allocator.nr_pools) {
+		SLS_ERR("cunit %u doesn't exist.\n", req.cunit);
 		return -EINVAL;
 	}
 
@@ -331,14 +333,14 @@ int deallocate_memory_unsafe(struct sls_memory_alloc_request req)
 	 * valid allocations in here, or calling into process manager
 	 * before this call.
 	 */
-	if (unlikely(!gen_pool_has_addr(allocator.pools[req.rank],
-					req.rank_offset, req.size))) {
+	if (unlikely(!gen_pool_has_addr(allocator.pools[req.cunit],
+					req.cunit_offset, req.size))) {
 		SLS_ERR("Deallocating a nonexistent object at %llu of size %llu from pool[%u]\n",
-			req.rank_offset, req.size, req.rank);
+			req.cunit_offset, req.size, req.cunit);
 		return -EINVAL;
 	}
 
-	gen_pool_free(allocator.pools[req.rank], req.rank_offset, req.size);
+	gen_pool_free(allocator.pools[req.cunit], req.cunit_offset, req.size);
 
 	return 0;
 }
@@ -376,7 +378,7 @@ static int deallocate_memory_ioctl(struct file *filp, struct allocator *alloc,
 	return err;
 }
 
-uint64_t get_total_size(uint8_t rank)
+uint64_t get_total_size(uint8_t cunit)
 {
 	struct gen_pool *pool = NULL;
 	uint64_t size = 0;
@@ -384,7 +386,7 @@ uint64_t get_total_size(uint8_t rank)
 	lock_sls_allocator();
 	{
 		ASSERT_EXCLUSIVE_ACCESS_SCOPED(allocator);
-		pool = allocator.pools[rank];
+		pool = allocator.pools[cunit];
 		size = pool ? gen_pool_size(pool) : 0;
 	}
 	unlock_sls_allocator();
@@ -392,7 +394,7 @@ uint64_t get_total_size(uint8_t rank)
 	return size;
 }
 
-uint64_t get_free_size(uint8_t rank)
+uint64_t get_free_size(uint8_t cunit)
 {
 	struct gen_pool *pool = NULL;
 	uint64_t size = 0;
@@ -400,7 +402,7 @@ uint64_t get_free_size(uint8_t rank)
 	lock_sls_allocator();
 	{
 		ASSERT_EXCLUSIVE_ACCESS_SCOPED(allocator);
-		pool = allocator.pools[rank];
+		pool = allocator.pools[cunit];
 		size = pool ? gen_pool_avail(pool) : 0;
 	}
 	unlock_sls_allocator();
diff --git a/drivers/pnm/sls_resource/allocator.h b/drivers/pnm/sls_resource/allocator.h
index 9b7d1e842..decd7d383 100644
--- a/drivers/pnm/sls_resource/allocator.h
+++ b/drivers/pnm/sls_resource/allocator.h
@@ -25,8 +25,8 @@ int deallocate_memory_unsafe(struct sls_memory_alloc_request req);
 void lock_sls_allocator(void);
 void unlock_sls_allocator(void);
 
-uint64_t get_total_size(uint8_t rank);
+uint64_t get_total_size(uint8_t cunit);
 
-uint64_t get_free_size(uint8_t rank);
+uint64_t get_free_size(uint8_t cunit);
 
 #endif /* __SLS_ALLOCATOR_H__ */
diff --git a/drivers/pnm/sls_resource/cunit_scheduler.c b/drivers/pnm/sls_resource/cunit_scheduler.c
new file mode 100644
index 000000000..1572274d5
--- /dev/null
+++ b/drivers/pnm/sls_resource/cunit_scheduler.c
@@ -0,0 +1,359 @@
+// SPDX-License-Identifier: GPL-2.0
+/* Copyright(c) 2022 Samsung LTD. All rights reserved. */
+
+#include "cunit_scheduler.h"
+#include "log.h"
+#include "process_manager.h"
+#include "topo/params.h"
+
+#include <linux/bitops.h>
+#include <linux/slab.h>
+#include <linux/uaccess.h>
+#include <linux/wait.h>
+
+#define BUSY_STATE (1)
+#define IDLE_STATE (0)
+
+/* this timeout value was tunned for FPGA, in order to avoid hangs, which
+ * presumably come from userspace cunit acquire attempts in a loop
+ */
+#define RETRY_TIMEOUT_NS (100000)
+
+struct cunit_scheduler {
+	/* struct for managing cunit read and write status */
+	struct sls_cunit_stat_t {
+		uint8_t wr_stat;
+		uint8_t cacheline_padding_1[L1_CACHE_BYTES - sizeof(uint8_t)];
+		uint8_t rd_stat;
+		uint8_t cacheline_padding_2[L1_CACHE_BYTES - sizeof(uint8_t)];
+		atomic64_t wr_acquisition_count;
+		uint8_t cacheline_padding_3[L1_CACHE_BYTES - sizeof(atomic64_t)];
+	} *cunit_stats;
+	atomic64_t retry_timeout_ns;
+	uint8_t cacheline_padding_4[L1_CACHE_BYTES - sizeof(atomic64_t)];
+	struct mutex cunit_stat_lock;
+};
+
+static struct cunit_scheduler sls_cunit_sched;
+
+static atomic_t wr_flag = ATOMIC_INIT(0);
+static atomic_t rd_flag = ATOMIC_INIT(0);
+static DECLARE_WAIT_QUEUE_HEAD(wr_wq);
+static DECLARE_WAIT_QUEUE_HEAD(rd_wq);
+
+static unsigned long
+acquire_free_cunit_for_write(struct cunit_scheduler *cunit_sched,
+			     unsigned int rw_msk, unsigned int wo_msk)
+{
+	unsigned long cunit;
+	struct sls_cunit_stat_t *rs = cunit_sched->cunit_stats;
+
+	cunit = __ffs(rw_msk ? rw_msk : wo_msk);
+
+	rs[cunit].wr_stat = BUSY_STATE;
+	atomic64_inc(&rs[cunit].wr_acquisition_count);
+	return cunit;
+}
+
+static void find_free_cunits(struct cunit_scheduler *cunit_sched,
+			     unsigned int msk, unsigned int *rw_msk,
+			     unsigned int *wo_msk)
+{
+	size_t cunit;
+	struct sls_cunit_stat_t *rs = cunit_sched->cunit_stats;
+
+	/* fill wo_msk and rw_msk according to actual read and write
+	 * cunit statuses and user requested mask
+	 */
+	for (cunit = 0; cunit < sls_topo()->nr_cunits; cunit++) {
+		if (msk & (1 << cunit) && rs[cunit].wr_stat == IDLE_STATE) {
+			if (rs[cunit].rd_stat == IDLE_STATE)
+				*rw_msk |= (1 << cunit);
+			else
+				*wo_msk |= (1 << cunit);
+		}
+	}
+
+	SLS_DBG("Find free cunits: rw = 0x%x, wo = 0x%x for mask = 0x%x",
+		*rw_msk, *wo_msk, msk);
+}
+
+void reset_cunit_scheduler(void)
+{
+	size_t cunit;
+
+	SLS_DBG("Resetting SLS cunit scheduler\n");
+
+	/* set all cunits as idle */
+	for (cunit = 0; cunit < sls_topo()->nr_cunits; cunit++) {
+		sls_cunit_sched.cunit_stats[cunit].wr_stat = IDLE_STATE;
+		sls_cunit_sched.cunit_stats[cunit].rd_stat = IDLE_STATE;
+		atomic64_set(
+			&sls_cunit_sched.cunit_stats[cunit].wr_acquisition_count,
+			0);
+	}
+
+	atomic64_set(&sls_cunit_sched.retry_timeout_ns, RETRY_TIMEOUT_NS);
+}
+
+int init_cunit_scheduler(void)
+{
+	sls_cunit_sched.cunit_stats = kcalloc(sls_topo()->nr_cunits,
+					      sizeof(struct sls_cunit_stat_t),
+					      GFP_KERNEL);
+
+	if (!sls_cunit_sched.cunit_stats) {
+		SLS_ERR("No free memory for cunit_stats\n");
+		return -ENOMEM;
+	}
+
+	atomic64_set(&sls_cunit_sched.retry_timeout_ns, RETRY_TIMEOUT_NS);
+	mutex_init(&sls_cunit_sched.cunit_stat_lock);
+
+	return 0;
+}
+
+void destroy_cunit_scheduler(void)
+{
+	kfree(sls_cunit_sched.cunit_stats);
+	mutex_destroy(&sls_cunit_sched.cunit_stat_lock);
+}
+
+static int get_cunit_write(struct cunit_scheduler *cunit_sched,
+			   unsigned int msk)
+{
+	int ret = -1;
+	/* masks for saving available cunits in corresponding bits */
+	unsigned int rw_mask = 0;
+	unsigned int wo_mask = 0;
+	unsigned int max_msk_val = (1 << sls_topo()->nr_cunits) - 1;
+
+	SLS_DBG("Acquiring cunit for write (mask 0x%x)\n", msk);
+
+	if (msk > max_msk_val) {
+		SLS_ERR("Invalid mask value: 0x%x\n", msk);
+		return ret;
+	}
+
+	mutex_lock(&cunit_sched->cunit_stat_lock);
+	{
+		ASSERT_EXCLUSIVE_ACCESS_SCOPED(cunit_sched->cunit_stats);
+		find_free_cunits(cunit_sched, msk, &rw_mask, &wo_mask);
+
+		if (rw_mask || wo_mask) {
+			ret = acquire_free_cunit_for_write(cunit_sched, rw_mask,
+							   wo_mask);
+		}
+	}
+	mutex_unlock(&cunit_sched->cunit_stat_lock);
+
+	if (ret < 0)
+		SLS_DBG("No free cunit for write\n");
+	else
+		SLS_DBG("Acquired cunit %d for write\n", ret);
+
+	return ret;
+}
+
+static int get_cunit_read(struct cunit_scheduler *cunit_sched,
+			  unsigned int cunit_id)
+{
+	int ret = -1;
+
+	SLS_DBG("Acquiring cunit %u for read\n", cunit_id);
+
+	if (cunit_id > (sls_topo()->nr_cunits - 1)) {
+		SLS_ERR("Invalid cunit value: %u\n", cunit_id);
+		return ret;
+	}
+
+	mutex_lock(&cunit_sched->cunit_stat_lock);
+	{
+		ASSERT_EXCLUSIVE_ACCESS_SCOPED(cunit_sched->cunit_stats);
+		if (cunit_sched->cunit_stats[cunit_id].rd_stat == IDLE_STATE) {
+			ret = cunit_id;
+			cunit_sched->cunit_stats[cunit_id].rd_stat = BUSY_STATE;
+		}
+	}
+	mutex_unlock(&cunit_sched->cunit_stat_lock);
+
+	if (ret < 0)
+		SLS_DBG("Could not acquire cunit %u for read\n", cunit_id);
+	else
+		SLS_DBG("Acquired cunit %u for read\n", cunit_id);
+
+	return ret;
+}
+
+int release_cunit_write(unsigned int cunit_id)
+{
+	int ret = cunit_id;
+
+	SLS_DBG("Releasing cunit %u for write\n", cunit_id);
+
+	if (cunit_id > (sls_topo()->nr_cunits - 1)) {
+		SLS_ERR("Invalid cunit value: %u\n", cunit_id);
+		return -1;
+	}
+
+	mutex_lock(&sls_cunit_sched.cunit_stat_lock);
+	{
+		ASSERT_EXCLUSIVE_ACCESS_SCOPED(sls_cunit_sched.cunit_stats);
+		sls_cunit_sched.cunit_stats[cunit_id].wr_stat = IDLE_STATE;
+	}
+	mutex_unlock(&sls_cunit_sched.cunit_stat_lock);
+
+	return ret;
+}
+
+int release_cunit_read(unsigned int cunit_id)
+{
+	int ret = cunit_id;
+
+	SLS_DBG("Releasing cunit %u for read\n", cunit_id);
+
+	if (cunit_id > (sls_topo()->nr_cunits - 1)) {
+		SLS_ERR("Invalid cunit value: %u\n", cunit_id);
+		return -1;
+	}
+
+	mutex_lock(&sls_cunit_sched.cunit_stat_lock);
+	{
+		ASSERT_EXCLUSIVE_ACCESS_SCOPED(sls_cunit_sched.cunit_stats);
+		sls_cunit_sched.cunit_stats[cunit_id].rd_stat = IDLE_STATE;
+	}
+	mutex_unlock(&sls_cunit_sched.cunit_stat_lock);
+	return ret;
+}
+
+// Here we disable kcsan checks due to unavoidable data race in
+// wait_event_interruptible_hrtimeout. We can't replace that function because the
+// HW is hanging
+static __no_kcsan int wait_for_cunit(struct cunit_scheduler *cunit_sched,
+				     atomic_t *flag, struct wait_queue_head *wq)
+{
+	atomic_set(flag, 0);
+	/* timeout in nanoseconds */
+	return wait_event_interruptible_hrtimeout(
+		*wq, atomic_read(flag),
+		atomic64_read(&cunit_sched->retry_timeout_ns));
+}
+
+int get_retry_read(struct cunit_scheduler *cunit_sched, unsigned int arg)
+{
+	if (wait_for_cunit(cunit_sched, &rd_flag, &rd_wq) >= 0)
+		return get_cunit_read(cunit_sched, arg);
+
+	return -1;
+}
+
+int get_retry_write(struct cunit_scheduler *cunit_sched, unsigned int arg)
+{
+	if (wait_for_cunit(cunit_sched, &wr_flag, &wr_wq) >= 0)
+		return get_cunit_write(cunit_sched, arg);
+
+	return -1;
+}
+int get_cunit_retry(struct cunit_scheduler *cunit_sched, unsigned int cmd,
+		    unsigned int arg)
+{
+	if (cmd == GET_CUNIT_FOR_READ)
+		return get_retry_read(cunit_sched, arg);
+	else if (cmd == GET_CUNIT_FOR_WRITE)
+		return get_retry_write(cunit_sched, arg);
+
+	SLS_ERR("Unknown cunit operation cmd [%d]\n", cmd);
+	return -EINVAL;
+}
+
+static int get_cunit(struct cunit_scheduler *cunit_sched, unsigned int cmd,
+		     unsigned int arg)
+{
+	if (cmd == GET_CUNIT_FOR_WRITE)
+		return get_cunit_write(cunit_sched, arg);
+	if (cmd == GET_CUNIT_FOR_READ)
+		return get_cunit_read(cunit_sched, arg);
+
+	SLS_ERR("Unknown cunit operation cmd [%d]\n", cmd);
+	return -EINVAL;
+}
+
+int get_sls_cunit(struct file *filp, unsigned int cmd, unsigned int arg)
+{
+	int ret = get_cunit(&sls_cunit_sched, cmd, arg);
+
+	if (ret < 0) /* was not able to get cunit for write, retry */
+		ret = get_cunit_retry(&sls_cunit_sched, cmd, arg);
+
+	if (ret >= 0) /* finally got cunit, add to process resources */
+		sls_proc_register_cunit(filp, ret, cmd);
+
+	return ret;
+}
+
+bool cunit_scheduler_cunit_state(uint8_t cunit)
+{
+	bool state;
+
+	mutex_lock(&sls_cunit_sched.cunit_stat_lock);
+	state = sls_cunit_sched.cunit_stats[cunit].wr_stat == BUSY_STATE;
+	mutex_unlock(&sls_cunit_sched.cunit_stat_lock);
+	return state;
+}
+
+uint64_t cunit_scheduler_acquisition_count(uint8_t cunit)
+{
+	struct sls_cunit_stat_t *axd_rs = sls_cunit_sched.cunit_stats;
+
+	return atomic64_read(&axd_rs[cunit].wr_acquisition_count);
+}
+
+uint64_t cunit_scheduler_acquisition_timeout(void)
+{
+	return atomic64_read(&sls_cunit_sched.retry_timeout_ns);
+}
+
+void cunit_scheduler_set_acquisition_timeout(uint64_t timeout)
+{
+	atomic64_set(&sls_cunit_sched.retry_timeout_ns, timeout);
+}
+
+static void wakeup_queue(struct cunit_scheduler *cunit_sched, int cunit,
+			 atomic_t *flag, struct wait_queue_head *wq)
+{
+	if (cunit >= 0) { /* wake up someone */
+		atomic_set(flag, 1);
+		wake_up_interruptible(wq);
+	}
+}
+
+static int release_and_wakeup(struct cunit_scheduler *cunit_sched,
+			      unsigned int cmd, unsigned int arg)
+{
+	int cunit;
+
+	if (cmd == RELEASE_WRITE_CUNIT) {
+		cunit = release_cunit_write(arg);
+		wakeup_queue(cunit_sched, cunit, &wr_flag, &wr_wq);
+	} else if (cmd == RELEASE_READ_CUNIT) {
+		cunit = release_cunit_read(arg);
+		wakeup_queue(cunit_sched, cunit, &rd_flag, &rd_wq);
+	} else {
+		SLS_ERR("Unknown cunit operation cmd [%d]\n", cmd);
+		return -EINVAL;
+	}
+
+	return cunit;
+}
+
+int release_sls_cunit(struct file *filp, unsigned int cmd, unsigned int arg)
+{
+	int cunit;
+
+	cunit = release_and_wakeup(&sls_cunit_sched, cmd, arg);
+
+	if (cunit >= 0)
+		return sls_proc_remove_cunit(filp, cunit, cmd);
+
+	return -1;
+}
diff --git a/drivers/pnm/sls_resource/cunit_scheduler.h b/drivers/pnm/sls_resource/cunit_scheduler.h
new file mode 100644
index 000000000..1f7967eb6
--- /dev/null
+++ b/drivers/pnm/sls_resource/cunit_scheduler.h
@@ -0,0 +1,30 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/* Copyright(c) 2022 Samsung LTD. All rights reserved. */
+
+#ifndef __SLS_RANK_SCHEDULER_H__
+#define __SLS_RANK_SCHEDULER_H__
+
+#include <linux/fs.h>
+#include <linux/sls_resources.h>
+#include <linux/types.h>
+
+void reset_cunit_scheduler(void);
+int init_cunit_scheduler(void);
+void destroy_cunit_scheduler(void);
+int get_sls_cunit(struct file *filp, unsigned int cmd, unsigned int arg);
+int release_sls_cunit(struct file *filp, unsigned int cmd, unsigned int arg);
+
+/* these functions are intended for direct cunit status manipulation from
+ * resource manager, in order to free resources which were not freed by
+ * user space process itself
+ */
+int release_cunit_write(unsigned int cunit_id);
+int release_cunit_read(unsigned int cunit_id);
+
+bool cunit_scheduler_cunit_state(uint8_t cunit);
+uint64_t cunit_scheduler_acquisition_count(uint8_t cunit);
+
+uint64_t cunit_scheduler_acquisition_timeout(void);
+void cunit_scheduler_set_acquisition_timeout(uint64_t timeout);
+
+#endif
diff --git a/drivers/pnm/sls_resource/process_manager.c b/drivers/pnm/sls_resource/process_manager.c
index 66d587a8c..dda5df152 100644
--- a/drivers/pnm/sls_resource/process_manager.c
+++ b/drivers/pnm/sls_resource/process_manager.c
@@ -3,8 +3,8 @@
 
 #include "process_manager.h"
 #include "allocator.h"
+#include "cunit_scheduler.h"
 #include "log.h"
-#include "rank_scheduler.h"
 
 #include <linux/sched.h>
 #include <linux/sched/signal.h>
@@ -21,8 +21,8 @@ struct sls_proc_desc {
 /* resources allocated for particular process */
 struct sls_proc_resources {
 	struct rb_root alloc_desc_tree;
-	unsigned long rank_write_mask;
-	unsigned long rank_read_mask;
+	unsigned long cunit_write_mask;
+	unsigned long cunit_read_mask;
 	/* this counter needed because release f_op is called on each close
 	 * syscall, sometimes userspace process can just open/close device
 	 * for reset ioctl purpose. take in mind this counter should be accessed
@@ -56,17 +56,17 @@ static struct process_manager proc_mgr = {
 	.proc_list_lock = __MUTEX_INITIALIZER(proc_mgr.proc_list_lock)
 };
 
-enum rank_access_type {
-	RANK_ACCESS_RD,
-	RANK_ACCESS_WR,
-	RANK_ACCESS_INVAL,
+enum cunit_access_type {
+	CUNIT_ACCESS_RD,
+	CUNIT_ACCESS_WR,
+	CUNIT_ACCESS_INVAL,
 };
 
 static inline bool sls_req_less(struct sls_memory_alloc_request a,
 				struct sls_memory_alloc_request b)
 {
-	return a.rank < b.rank ||
-	       (a.rank == b.rank && a.rank_offset < b.rank_offset);
+	return a.cunit < b.cunit ||
+	       (a.cunit == b.cunit && a.cunit_offset < b.cunit_offset);
 }
 
 static bool desc_delete(struct rb_root *root,
@@ -115,19 +115,19 @@ static bool desc_insert(struct rb_root *root, struct sls_proc_desc *proc_desc)
 	return true;
 }
 
-static inline enum rank_access_type get_rank_access_type(unsigned int cmd)
+static inline enum cunit_access_type get_cunit_access_type(unsigned int cmd)
 {
-	if (cmd == GET_RANK_FOR_WRITE || cmd == RELEASE_WRITE_RANK)
-		return RANK_ACCESS_WR;
-	if (cmd == GET_RANK_FOR_READ || cmd == RELEASE_READ_RANK)
-		return RANK_ACCESS_RD;
+	if (cmd == GET_CUNIT_FOR_WRITE || cmd == RELEASE_WRITE_CUNIT)
+		return CUNIT_ACCESS_WR;
+	if (cmd == GET_CUNIT_FOR_READ || cmd == RELEASE_READ_CUNIT)
+		return CUNIT_ACCESS_RD;
 
-	return RANK_ACCESS_INVAL;
+	return CUNIT_ACCESS_INVAL;
 }
 
-static inline const char *rank_access_type_to_str(enum rank_access_type type)
+static inline const char *cunit_access_type_to_str(enum cunit_access_type type)
 {
-	return type == RANK_ACCESS_RD ? "rd" : "wr";
+	return type == CUNIT_ACCESS_RD ? "rd" : "wr";
 }
 
 static pid_t get_current_process_id(void)
@@ -141,7 +141,7 @@ static pid_t get_current_process_id(void)
 
 static bool has_resources_leaked(struct sls_proc_resources *proc_res)
 {
-	return proc_res->rank_write_mask || proc_res->rank_read_mask ||
+	return proc_res->cunit_write_mask || proc_res->cunit_read_mask ||
 	       !RB_EMPTY_ROOT(&proc_res->alloc_desc_tree);
 }
 
@@ -154,33 +154,33 @@ static void track_leaked_resources(struct process_manager *mgr,
 	atomic64_inc(&mgr->leaked);
 	list_add(&proc_res->list, &mgr->leaked_process_list);
 
-	SLS_DBG("Tracked leakage by pid: %d, tid: %d; rank_rw_mask: %lu, rank_rd_mask: %lu\n",
+	SLS_DBG("Tracked leakage by pid: %d, tid: %d; cunit_rw_mask: %lu, cunit_rd_mask: %lu\n",
 		get_current_process_id(), current->pid,
-		proc_res->rank_write_mask, proc_res->rank_read_mask);
+		proc_res->cunit_write_mask, proc_res->cunit_read_mask);
 
 	for (node = rb_first(&proc_res->alloc_desc_tree); node;
 	     node = rb_next(node)) {
 		desc = rb_entry(node, struct sls_proc_desc, node);
-		SLS_DBG("Leaked memory under desc[rank = %u, rank_offset = %llu]\n",
-			desc->request.rank, desc->request.rank_offset);
+		SLS_DBG("Leaked memory under desc[cunit = %u, cunit_offset = %llu]\n",
+			desc->request.cunit, desc->request.cunit_offset);
 	}
 }
 
-static bool release_rank_mask(unsigned long *rank_mask,
-			      enum rank_access_type type)
+static bool release_cunit_mask(unsigned long *cunit_mask,
+			       enum cunit_access_type type)
 {
 	bool failed = false;
 	unsigned long bit_index;
 
-	while (*rank_mask) {
-		bit_index = ffs(*rank_mask) - 1;
-		if (type == RANK_ACCESS_WR)
-			failed |= (release_rank_write(bit_index) != bit_index);
+	while (*cunit_mask) {
+		bit_index = ffs(*cunit_mask) - 1;
+		if (type == CUNIT_ACCESS_WR)
+			failed |= (release_cunit_write(bit_index) != bit_index);
 		else
-			failed |= (release_rank_read(bit_index) != bit_index);
-		clear_bit(bit_index, rank_mask);
-		SLS_INF("Abnormal release rank_%s[%lu], pid: %d, tid: %d\n",
-			rank_access_type_to_str(type), bit_index,
+			failed |= (release_cunit_read(bit_index) != bit_index);
+		clear_bit(bit_index, cunit_mask);
+		SLS_INF("Abnormal release cunit_%s[%lu], pid: %d, tid: %d\n",
+			cunit_access_type_to_str(type), bit_index,
 			get_current_process_id(), current->pid);
 	}
 
@@ -196,9 +196,11 @@ static int release_process_resources(struct sls_proc_resources *proc_res)
 	struct rb_node *next;
 	bool failed = false;
 
-	/* handle unreleased ranks */
-	failed |= release_rank_mask(&proc_res->rank_write_mask, RANK_ACCESS_WR);
-	failed |= release_rank_mask(&proc_res->rank_read_mask, RANK_ACCESS_RD);
+	/* handle unreleased cunits */
+	failed |= release_cunit_mask(&proc_res->cunit_write_mask,
+				     CUNIT_ACCESS_WR);
+	failed |=
+		release_cunit_mask(&proc_res->cunit_read_mask, CUNIT_ACCESS_RD);
 
 	/* handle unreleased allocation descriptors */
 	while ((next = rb_first(&proc_res->alloc_desc_tree))) {
@@ -218,9 +220,9 @@ static int release_process_resources(struct sls_proc_resources *proc_res)
 
 			failed |= deallocate_memory_unsafe(
 					  desc_node->request) != 0;
-			SLS_INF("Abnormal release desc[rank=%u, rank_offset=%llu],pid:%d,tid:%d\n",
-				desc_node->request.rank,
-				desc_node->request.rank_offset,
+			SLS_INF("Abnormal release desc[cunit=%u, offset=%llu],pid:%d,tid:%d\n",
+				desc_node->request.cunit,
+				desc_node->request.cunit_offset,
 				get_current_process_id(), current->pid);
 			next = rb_first(&proc_res->alloc_desc_tree);
 			rb_erase(&desc_node->node, &proc_res->alloc_desc_tree);
@@ -289,10 +291,10 @@ int release_sls_process(struct file *filp)
 	return err_code;
 }
 
-static int set_rank_status(struct file *filp, struct process_manager *mgr,
-			   enum rank_access_type type, int rank, bool set)
+static int set_cunit_status(struct file *filp, struct process_manager *mgr,
+			    enum cunit_access_type type, int cunit, bool set)
 {
-	unsigned long *rank_mask;
+	unsigned long *cunit_mask;
 	struct sls_proc_resources *proc_res = NULL;
 
 	if (!filp)
@@ -306,51 +308,52 @@ static int set_rank_status(struct file *filp, struct process_manager *mgr,
 	mutex_lock(&proc_res->sls_proc_lock);
 	{
 		/* get read or write mask according to request */
-		rank_mask = type == RANK_ACCESS_RD ? &proc_res->rank_read_mask :
-						     &proc_res->rank_write_mask;
-		/* set or unset rank's bit according to request */
-		assign_bit(rank, rank_mask, set);
+		cunit_mask = type == CUNIT_ACCESS_RD ?
+				     &proc_res->cunit_read_mask :
+				     &proc_res->cunit_write_mask;
+		/* set or unset cunit's bit according to request */
+		assign_bit(cunit, cunit_mask, set);
 	}
 	mutex_unlock(&proc_res->sls_proc_lock);
 	return 0;
 }
 
-int sls_proc_register_rank(struct file *filp, int rank, unsigned int cmd)
+int sls_proc_register_cunit(struct file *filp, int cunit, unsigned int cmd)
 {
-	enum rank_access_type rank_type = get_rank_access_type(cmd);
+	enum cunit_access_type cunit_type = get_cunit_access_type(cmd);
 
-	if (unlikely(rank_type == RANK_ACCESS_INVAL)) {
-		SLS_ERR("Unknown rank type\n");
+	if (unlikely(cunit_type == CUNIT_ACCESS_INVAL)) {
+		SLS_ERR("Unknown cunit type\n");
 		return -1;
 	}
 
-	SLS_DBG("Registering rank_%s[%d], pid: %d, tid: %d\n",
-		rank_access_type_to_str(rank_type), rank,
+	SLS_DBG("Registering cunit_%s[%d], pid: %d, tid: %d\n",
+		cunit_access_type_to_str(cunit_type), cunit,
 		get_current_process_id(), current->pid);
-	if (set_rank_status(filp, &proc_mgr, rank_type, rank, true)) {
-		SLS_ERR("Fail to register rank_%s[%d], pid: %d, tid: %d\n",
-			rank_access_type_to_str(rank_type), rank,
+	if (set_cunit_status(filp, &proc_mgr, cunit_type, cunit, true)) {
+		SLS_ERR("Fail to register cunit_%s[%d], pid: %d, tid: %d\n",
+			cunit_access_type_to_str(cunit_type), cunit,
 			get_current_process_id(), current->pid);
 		return -1;
 	}
 	return 0;
 }
 
-int sls_proc_remove_rank(struct file *filp, int rank, unsigned int cmd)
+int sls_proc_remove_cunit(struct file *filp, int cunit, unsigned int cmd)
 {
-	int rank_type = get_rank_access_type(cmd);
+	int cunit_type = get_cunit_access_type(cmd);
 
-	if (unlikely(rank_type == RANK_ACCESS_INVAL)) {
-		SLS_ERR("Unknown rank type\n");
+	if (unlikely(cunit_type == CUNIT_ACCESS_INVAL)) {
+		SLS_ERR("Unknown cunit type\n");
 		return -1;
 	}
 
-	SLS_DBG("Removing rank_%s[%d], pid: %d, tid: %d\n",
-		rank_access_type_to_str(rank_type), rank,
+	SLS_DBG("Removing cunit_%s[%d], pid: %d, tid: %d\n",
+		cunit_access_type_to_str(cunit_type), cunit,
 		get_current_process_id(), current->pid);
-	if (set_rank_status(filp, &proc_mgr, rank_type, rank, false)) {
-		SLS_ERR("Fail to remove rank_%s[%d], pid: %d, tid: %d\n",
-			rank_access_type_to_str(rank_type), rank,
+	if (set_cunit_status(filp, &proc_mgr, cunit_type, cunit, false)) {
+		SLS_ERR("Fail to remove cunit_%s[%d], pid: %d, tid: %d\n",
+			cunit_access_type_to_str(cunit_type), cunit,
 			get_current_process_id(), current->pid);
 		return -1;
 	}
@@ -412,12 +415,12 @@ static int unregister_alloc_request(struct file *filp,
 int sls_proc_register_alloc(struct file *filp,
 			    struct sls_memory_alloc_request req)
 {
-	SLS_DBG("Registering allocation, desc[rank = %u, rank_offset = %llu], pid: %d, tid: %d\n",
-		req.rank, req.rank_offset, get_current_process_id(),
+	SLS_DBG("Registering allocation, desc[cunit = %u, cunit_offset = %llu], pid: %d, tid: %d\n",
+		req.cunit, req.cunit_offset, get_current_process_id(),
 		current->pid);
 	if (register_alloc_request(filp, &proc_mgr, req)) {
-		SLS_ERR("Fail to register rank: %u, rank_offset: %llu, pid: %d, tid: %d\n",
-			req.rank, req.rank_offset, get_current_process_id(),
+		SLS_ERR("Fail to register cunit: %u, cunit_offset: %llu, pid: %d, tid: %d\n",
+			req.cunit, req.cunit_offset, get_current_process_id(),
 			current->pid);
 		return -1;
 	}
@@ -427,12 +430,12 @@ int sls_proc_register_alloc(struct file *filp,
 int sls_proc_remove_alloc(struct file *filp,
 			  struct sls_memory_alloc_request req)
 {
-	SLS_DBG("Removing allocation, desc[rank = %u, rank_offset = %llu], pid: %d, tid: %d\n",
-		req.rank, req.rank_offset, get_current_process_id(),
+	SLS_DBG("Removing allocation, desc[cunit = %u, cunit_offset = %llu], pid: %d, tid: %d\n",
+		req.cunit, req.cunit_offset, get_current_process_id(),
 		current->pid);
 	if (unregister_alloc_request(filp, &proc_mgr, req)) {
-		SLS_ERR("Fail to remove rank: %u, rank_offset: %llu, pid: %d, tid: %d\n",
-			req.rank, req.rank_offset, get_current_process_id(),
+		SLS_ERR("Fail to remove cunit: %u, cunit_offset: %llu, pid: %d, tid: %d\n",
+			req.cunit, req.cunit_offset, get_current_process_id(),
 			current->pid);
 		return -1;
 	}
diff --git a/drivers/pnm/sls_resource/process_manager.h b/drivers/pnm/sls_resource/process_manager.h
index eaae0f773..739aff17f 100644
--- a/drivers/pnm/sls_resource/process_manager.h
+++ b/drivers/pnm/sls_resource/process_manager.h
@@ -5,7 +5,7 @@
 #define __SLS_PROCESS_MANAGER_H__
 
 #include "allocator.h"
-#include "rank_scheduler.h"
+#include "cunit_scheduler.h"
 
 #include <linux/fs.h>
 #include <linux/list.h>
@@ -22,12 +22,12 @@ int release_sls_process(struct file *filp);
 /* functions for adding allocation/cunits into process's resources data structure*/
 int sls_proc_register_alloc(struct file *filp,
 			    struct sls_memory_alloc_request req);
-int sls_proc_register_rank(struct file *filp, int rank, unsigned int cmd);
+int sls_proc_register_cunit(struct file *filp, int cunit, unsigned int cmd);
 
 /* function for removing allocation/cunits from process's resources data structure*/
 int sls_proc_remove_alloc(struct file *filp,
 			  struct sls_memory_alloc_request req);
-int sls_proc_remove_rank(struct file *filp, int rank, unsigned int cmd);
+int sls_proc_remove_cunit(struct file *filp, int cunit, unsigned int cmd);
 
 int sls_proc_manager_cleanup_on(void);
 void sls_proc_manager_cleanup_off(void);
diff --git a/drivers/pnm/sls_resource/rank_scheduler.c b/drivers/pnm/sls_resource/rank_scheduler.c
deleted file mode 100644
index 44da47230..000000000
--- a/drivers/pnm/sls_resource/rank_scheduler.c
+++ /dev/null
@@ -1,357 +0,0 @@
-// SPDX-License-Identifier: GPL-2.0
-/* Copyright(c) 2022 Samsung LTD. All rights reserved. */
-
-#include "rank_scheduler.h"
-#include "log.h"
-#include "process_manager.h"
-#include "topo/params.h"
-
-#include <linux/bitops.h>
-#include <linux/slab.h>
-#include <linux/uaccess.h>
-#include <linux/wait.h>
-
-#define BUSY_STATE (1)
-#define IDLE_STATE (0)
-
-/* this timeout value was tunned for FPGA, in order to avoid hangs, which
- * presumably come from userspace rank acquire attempts in a loop
- */
-#define RETRY_TIMEOUT_NS (100000)
-
-struct rank_scheduler {
-	/* struct for managing rank read and write status */
-	struct sls_rank_stat_t {
-		uint8_t wr_stat;
-		uint8_t cacheline_padding_1[L1_CACHE_BYTES - sizeof(uint8_t)];
-		uint8_t rd_stat;
-		uint8_t cacheline_padding_2[L1_CACHE_BYTES - sizeof(uint8_t)];
-		atomic64_t wr_acquisition_count;
-		uint8_t cacheline_padding_3[L1_CACHE_BYTES - sizeof(atomic64_t)];
-	} *rank_stats;
-	atomic64_t retry_timeout_ns;
-	uint8_t cacheline_padding_4[L1_CACHE_BYTES - sizeof(atomic64_t)];
-	struct mutex rank_stat_lock;
-};
-
-static struct rank_scheduler sls_rank_sched;
-
-static atomic_t wr_flag = ATOMIC_INIT(0);
-static atomic_t rd_flag = ATOMIC_INIT(0);
-static DECLARE_WAIT_QUEUE_HEAD(wr_wq);
-static DECLARE_WAIT_QUEUE_HEAD(rd_wq);
-
-static unsigned long
-acquire_free_rank_for_write(struct rank_scheduler *rank_sched,
-			    unsigned int rw_msk, unsigned int wo_msk)
-{
-	unsigned long rank;
-	struct sls_rank_stat_t *rs = rank_sched->rank_stats;
-
-	rank = __ffs(rw_msk ? rw_msk : wo_msk);
-
-	rs[rank].wr_stat = BUSY_STATE;
-	atomic64_inc(&rs[rank].wr_acquisition_count);
-	return rank;
-}
-
-static void find_free_ranks(struct rank_scheduler *rank_sched, unsigned int msk,
-			    unsigned int *rw_msk, unsigned int *wo_msk)
-{
-	size_t rank;
-	struct sls_rank_stat_t *rs = rank_sched->rank_stats;
-
-	/* fill wo_msk and rw_msk according to actual read and write
-	 * rank statuses and user requested mask
-	 */
-	for (rank = 0; rank < sls_topo()->nr_cunits; rank++) {
-		if (msk & (1 << rank) && rs[rank].wr_stat == IDLE_STATE) {
-			if (rs[rank].rd_stat == IDLE_STATE)
-				*rw_msk |= (1 << rank);
-			else
-				*wo_msk |= (1 << rank);
-		}
-	}
-
-	SLS_DBG("Find free ranks: rw = 0x%x, wo = 0x%x for mask = 0x%x",
-		*rw_msk, *wo_msk, msk);
-}
-
-void reset_rank_scheduler(void)
-{
-	size_t rank;
-
-	SLS_DBG("Resetting SLS rank scheduler\n");
-
-	/* set all ranks as idle */
-	for (rank = 0; rank < sls_topo()->nr_cunits; rank++) {
-		sls_rank_sched.rank_stats[rank].wr_stat = IDLE_STATE;
-		sls_rank_sched.rank_stats[rank].rd_stat = IDLE_STATE;
-		atomic64_set(
-			&sls_rank_sched.rank_stats[rank].wr_acquisition_count,
-			0);
-	}
-
-	atomic64_set(&sls_rank_sched.retry_timeout_ns, RETRY_TIMEOUT_NS);
-}
-
-int init_rank_scheduler(void)
-{
-	sls_rank_sched.rank_stats = kcalloc(sls_topo()->nr_cunits,
-					    sizeof(struct sls_rank_stat_t),
-					    GFP_KERNEL);
-
-	if (!sls_rank_sched.rank_stats) {
-		SLS_ERR("No free memory for rank_stats\n");
-		return -ENOMEM;
-	}
-
-	atomic64_set(&sls_rank_sched.retry_timeout_ns, RETRY_TIMEOUT_NS);
-	mutex_init(&sls_rank_sched.rank_stat_lock);
-
-	return 0;
-}
-
-void destroy_rank_scheduler(void)
-{
-	kfree(sls_rank_sched.rank_stats);
-	mutex_destroy(&sls_rank_sched.rank_stat_lock);
-}
-
-static int get_rank_write(struct rank_scheduler *rank_sched, unsigned int msk)
-{
-	int ret = -1;
-	/* masks for saving available ranks in corresponding bits */
-	unsigned int rw_mask = 0;
-	unsigned int wo_mask = 0;
-	unsigned int max_msk_val = (1 << sls_topo()->nr_cunits) - 1;
-
-	SLS_DBG("Acquiring rank for write (mask 0x%x)\n", msk);
-
-	if (msk > max_msk_val) {
-		SLS_ERR("Invalid mask value: 0x%x\n", msk);
-		return ret;
-	}
-
-	mutex_lock(&rank_sched->rank_stat_lock);
-	{
-		ASSERT_EXCLUSIVE_ACCESS_SCOPED(rank_sched->rank_stats);
-		find_free_ranks(rank_sched, msk, &rw_mask, &wo_mask);
-
-		if (rw_mask || wo_mask) {
-			ret = acquire_free_rank_for_write(rank_sched, rw_mask,
-							  wo_mask);
-		}
-	}
-	mutex_unlock(&rank_sched->rank_stat_lock);
-
-	if (ret < 0)
-		SLS_DBG("No free rank for write\n");
-	else
-		SLS_DBG("Acquired rank %d for write\n", ret);
-
-	return ret;
-}
-
-static int get_rank_read(struct rank_scheduler *rank_sched,
-			 unsigned int rank_id)
-{
-	int ret = -1;
-
-	SLS_DBG("Acquiring rank %u for read\n", rank_id);
-
-	if (rank_id > (sls_topo()->nr_cunits - 1)) {
-		SLS_ERR("Invalid rank value: %u\n", rank_id);
-		return ret;
-	}
-
-	mutex_lock(&rank_sched->rank_stat_lock);
-	{
-		ASSERT_EXCLUSIVE_ACCESS_SCOPED(rank_sched->rank_stats);
-		if (rank_sched->rank_stats[rank_id].rd_stat == IDLE_STATE) {
-			ret = rank_id;
-			rank_sched->rank_stats[rank_id].rd_stat = BUSY_STATE;
-		}
-	}
-	mutex_unlock(&rank_sched->rank_stat_lock);
-
-	if (ret < 0)
-		SLS_DBG("Could not acquire rank %u for read\n", rank_id);
-	else
-		SLS_DBG("Acquired rank %u for read\n", rank_id);
-
-	return ret;
-}
-
-int release_rank_write(unsigned int rank_id)
-{
-	int ret = rank_id;
-
-	SLS_DBG("Releasing rank %u for write\n", rank_id);
-
-	if (rank_id > (sls_topo()->nr_cunits - 1)) {
-		SLS_ERR("Invalid rank value: %u\n", rank_id);
-		return -1;
-	}
-
-	mutex_lock(&sls_rank_sched.rank_stat_lock);
-	{
-		ASSERT_EXCLUSIVE_ACCESS_SCOPED(sls_rank_sched.rank_stats);
-		sls_rank_sched.rank_stats[rank_id].wr_stat = IDLE_STATE;
-	}
-	mutex_unlock(&sls_rank_sched.rank_stat_lock);
-
-	return ret;
-}
-
-int release_rank_read(unsigned int rank_id)
-{
-	int ret = rank_id;
-
-	SLS_DBG("Releasing rank %u for read\n", rank_id);
-
-	if (rank_id > (sls_topo()->nr_cunits - 1)) {
-		SLS_ERR("Invalid rank value: %u\n", rank_id);
-		return -1;
-	}
-
-	mutex_lock(&sls_rank_sched.rank_stat_lock);
-	{
-		ASSERT_EXCLUSIVE_ACCESS_SCOPED(sls_rank_sched.rank_stats);
-		sls_rank_sched.rank_stats[rank_id].rd_stat = IDLE_STATE;
-	}
-	mutex_unlock(&sls_rank_sched.rank_stat_lock);
-	return ret;
-}
-
-// Here we disable kcsan checks due to unavoidable data race in
-// wait_event_interruptible_hrtimeout. We can't replace that function because the
-// HW is hanging
-static __no_kcsan int wait_for_rank(struct rank_scheduler *rank_sched,
-				    atomic_t *flag, struct wait_queue_head *wq)
-{
-	atomic_set(flag, 0);
-	/* timeout in nanoseconds */
-	return wait_event_interruptible_hrtimeout(
-		*wq, atomic_read(flag),
-		atomic64_read(&rank_sched->retry_timeout_ns));
-}
-
-int get_retry_read(struct rank_scheduler *rank_sched, unsigned int arg)
-{
-	if (wait_for_rank(rank_sched, &rd_flag, &rd_wq) >= 0)
-		return get_rank_read(rank_sched, arg);
-
-	return -1;
-}
-
-int get_retry_write(struct rank_scheduler *rank_sched, unsigned int arg)
-{
-	if (wait_for_rank(rank_sched, &wr_flag, &wr_wq) >= 0)
-		return get_rank_write(rank_sched, arg);
-
-	return -1;
-}
-int get_rank_retry(struct rank_scheduler *rank_sched, unsigned int cmd,
-		   unsigned int arg)
-{
-	if (cmd == GET_RANK_FOR_READ)
-		return get_retry_read(rank_sched, arg);
-	else if (cmd == GET_RANK_FOR_WRITE)
-		return get_retry_write(rank_sched, arg);
-
-	SLS_ERR("Unknown rank operation cmd [%d]\n", cmd);
-	return -EINVAL;
-}
-
-static int get_rank(struct rank_scheduler *rank_sched, unsigned int cmd,
-		    unsigned int arg)
-{
-	if (cmd == GET_RANK_FOR_WRITE)
-		return get_rank_write(rank_sched, arg);
-	if (cmd == GET_RANK_FOR_READ)
-		return get_rank_read(rank_sched, arg);
-
-	SLS_ERR("Unknown rank operation cmd [%d]\n", cmd);
-	return -EINVAL;
-}
-
-int get_sls_rank(struct file *filp, unsigned int cmd, unsigned int arg)
-{
-	int ret = get_rank(&sls_rank_sched, cmd, arg);
-
-	if (ret < 0) /* was not able to get rank for write, retry */
-		ret = get_rank_retry(&sls_rank_sched, cmd, arg);
-
-	if (ret >= 0) /* finally got rank, add to process resources */
-		sls_proc_register_rank(filp, ret, cmd);
-
-	return ret;
-}
-
-bool rank_scheduler_rank_state(uint8_t rank)
-{
-	bool state;
-
-	mutex_lock(&sls_rank_sched.rank_stat_lock);
-	state = sls_rank_sched.rank_stats[rank].wr_stat == BUSY_STATE;
-	mutex_unlock(&sls_rank_sched.rank_stat_lock);
-	return state;
-}
-
-uint64_t rank_scheduler_acquisition_count(uint8_t rank)
-{
-	struct sls_rank_stat_t *axd_rs = sls_rank_sched.rank_stats;
-
-	return atomic64_read(&axd_rs[rank].wr_acquisition_count);
-}
-
-uint64_t rank_scheduler_acquisition_timeout(void)
-{
-	return atomic64_read(&sls_rank_sched.retry_timeout_ns);
-}
-
-void rank_scheduler_set_acquisition_timeout(uint64_t timeout)
-{
-	atomic64_set(&sls_rank_sched.retry_timeout_ns, timeout);
-}
-
-static void wakeup_queue(struct rank_scheduler *rank_sched, int rank,
-			 atomic_t *flag, struct wait_queue_head *wq)
-{
-	if (rank >= 0) { /* wake up someone */
-		atomic_set(flag, 1);
-		wake_up_interruptible(wq);
-	}
-}
-
-static int release_and_wakeup(struct rank_scheduler *rank_sched,
-			      unsigned int cmd, unsigned int arg)
-{
-	int rank;
-
-	if (cmd == RELEASE_WRITE_RANK) {
-		rank = release_rank_write(arg);
-		wakeup_queue(rank_sched, rank, &wr_flag, &wr_wq);
-	} else if (cmd == RELEASE_READ_RANK) {
-		rank = release_rank_read(arg);
-		wakeup_queue(rank_sched, rank, &rd_flag, &rd_wq);
-	} else {
-		SLS_ERR("Unknown rank operation cmd [%d]\n", cmd);
-		return -EINVAL;
-	}
-
-	return rank;
-}
-
-int release_sls_rank(struct file *filp, unsigned int cmd, unsigned int arg)
-{
-	int rank;
-
-	rank = release_and_wakeup(&sls_rank_sched, cmd, arg);
-
-	if (rank >= 0)
-		return sls_proc_remove_rank(filp, rank, cmd);
-
-	return -1;
-}
diff --git a/drivers/pnm/sls_resource/rank_scheduler.h b/drivers/pnm/sls_resource/rank_scheduler.h
deleted file mode 100644
index 3f439739d..000000000
--- a/drivers/pnm/sls_resource/rank_scheduler.h
+++ /dev/null
@@ -1,30 +0,0 @@
-/* SPDX-License-Identifier: GPL-2.0 */
-/* Copyright(c) 2022 Samsung LTD. All rights reserved. */
-
-#ifndef __SLS_RANK_SCHEDULER_H__
-#define __SLS_RANK_SCHEDULER_H__
-
-#include <linux/fs.h>
-#include <linux/sls_resources.h>
-#include <linux/types.h>
-
-void reset_rank_scheduler(void);
-int init_rank_scheduler(void);
-void destroy_rank_scheduler(void);
-int get_sls_rank(struct file *filp, unsigned int cmd, unsigned int arg);
-int release_sls_rank(struct file *filp, unsigned int cmd, unsigned int arg);
-
-/* these functions are intended for direct rank status manipulation from
- * resource manager, in order to free resources which were not freed by
- * user space process itself
- */
-int release_rank_write(unsigned int rank_id);
-int release_rank_read(unsigned int rank_id);
-
-bool rank_scheduler_rank_state(uint8_t rank);
-uint64_t rank_scheduler_acquisition_count(uint8_t rank);
-
-uint64_t rank_scheduler_acquisition_timeout(void);
-void rank_scheduler_set_acquisition_timeout(uint64_t timeout);
-
-#endif
diff --git a/drivers/pnm/sls_resource/sls.c b/drivers/pnm/sls_resource/sls.c
index 706292da5..06ce67512 100644
--- a/drivers/pnm/sls_resource/sls.c
+++ b/drivers/pnm/sls_resource/sls.c
@@ -72,7 +72,7 @@ long sls_ioctl(struct file *filp, unsigned int cmd, unsigned long __user arg)
 
 	switch (cmd) {
 	case DEVICE_IOCRESET:
-		reset_rank_scheduler();
+		reset_cunit_scheduler();
 		retval = reset_sls_allocator();
 		reset_process_manager();
 		break;
@@ -80,13 +80,13 @@ long sls_ioctl(struct file *filp, unsigned int cmd, unsigned long __user arg)
 	case DEALLOCATE_MEMORY:
 		retval = mem_process_ioctl(filp, cmd, arg);
 		break;
-	case GET_RANK_FOR_WRITE:
-	case GET_RANK_FOR_READ:
-		retval = get_sls_rank(filp, cmd, arg);
+	case GET_CUNIT_FOR_WRITE:
+	case GET_CUNIT_FOR_READ:
+		retval = get_sls_cunit(filp, cmd, arg);
 		break;
-	case RELEASE_READ_RANK:
-	case RELEASE_WRITE_RANK:
-		retval = release_sls_rank(filp, cmd, arg);
+	case RELEASE_READ_CUNIT:
+	case RELEASE_WRITE_CUNIT:
+		retval = release_sls_cunit(filp, cmd, arg);
 		break;
 	case NOP:
 		// Do nothing. We need it only for context switch
@@ -184,10 +184,10 @@ int init_sls_device(void)
 	if (err)
 		goto allocator_fail;
 
-	/* Reset ranks status and synchronization primitives */
-	err = init_rank_scheduler();
+	/* Reset cunits status and synchronization primitives */
+	err = init_cunit_scheduler();
 	if (err)
-		goto rank_scheduler_fail;
+		goto cunit_scheduler_fail;
 
 	/* Create sysfs subsystem for the device */
 	err = build_sls_sysfs(mem_cunit_info, mem_info, sls_resource_device);
@@ -198,9 +198,9 @@ int init_sls_device(void)
 	return 0;
 
 build_sysfs_fail:
-	destroy_rank_scheduler();
+	destroy_cunit_scheduler();
 	cleanup_process_manager();
-rank_scheduler_fail:
+cunit_scheduler_fail:
 	cleanup_sls_allocator();
 allocator_fail:
 	sls_destroy_mem_cunit_info(mem_cunit_info);
@@ -233,7 +233,7 @@ void cleanup_sls_device(void)
 	cleanup_process_manager();
 
 	/* Reset state */
-	destroy_rank_scheduler();
+	destroy_cunit_scheduler();
 
 	/* Remove test attribute */
 	destroy_sls_sysfs();
diff --git a/drivers/pnm/sls_resource/sysfs.c b/drivers/pnm/sls_resource/sysfs.c
index ad96529d3..37e40a094 100644
--- a/drivers/pnm/sls_resource/sysfs.c
+++ b/drivers/pnm/sls_resource/sysfs.c
@@ -3,10 +3,10 @@
 
 #include "sysfs.h"
 #include "allocator.h"
+#include "cunit_scheduler.h"
 #include "log.h"
 #include "private.h"
 #include "process_manager.h"
-#include "rank_scheduler.h"
 #include "topo/export.h"
 #include "topo/params.h"
 
@@ -19,7 +19,7 @@
 
 /* [TODO: @p.bred] Refactor all sysfs inc. error handling within MCS23-1373 */
 
-#define RANK_ATTR_MAX 4
+#define CUNIT_ATTR_MAX 4
 #define SYSFS_GROUPS_COUNT 1
 #define REGION_SYSFS_ATTR_COUNT 4
 #define RAW_REGION_SYSFS_ATTR_COUNT 3
@@ -27,7 +27,7 @@
 
 static const struct sls_mem_cunit_info *mem_cunit_info;
 static const struct sls_mem_info *mem_info;
-static struct kobject *ranks_kobj;
+static struct kobject *cunits_kobj;
 static struct kobject *mappings_kobj;
 
 static ssize_t leaked_show(struct device *device, struct device_attribute *attr,
@@ -71,7 +71,7 @@ static DEVICE_ATTR_RW(cleanup);
 static ssize_t acq_timeout_show(struct device *device,
 				struct device_attribute *attr, char *buf)
 {
-	uint64_t acquisition_count = rank_scheduler_acquisition_timeout();
+	uint64_t acquisition_count = cunit_scheduler_acquisition_timeout();
 
 	return sysfs_emit(buf, "%llu\n", acquisition_count);
 }
@@ -83,12 +83,12 @@ static ssize_t acq_timeout_store(struct device *device,
 	uint64_t acq_timeout;
 
 	if (kstrtoull(buf, 10, &acq_timeout)) {
-		SLS_ERR("Failed to convert rank acquisition timeout string ('%s') to integer.\n",
+		SLS_ERR("Failed to convert cunit acquisition timeout string ('%s') to integer.\n",
 			buf);
 		return -EINVAL;
 	}
 	SLS_DBG("Setting acq_timeout to %llu ns via sysfs\n", acq_timeout);
-	rank_scheduler_set_acquisition_timeout(acq_timeout);
+	cunit_scheduler_set_acquisition_timeout(acq_timeout);
 	return count;
 }
 static DEVICE_ATTR_RW(acq_timeout);
@@ -134,16 +134,16 @@ static const struct attribute_group *dev_attr_groups[] = {
 	NULL,
 };
 
-struct rank_attribute {
+struct cunit_attribute {
 	struct attribute attr;
-	ssize_t (*show)(struct rank_attribute *attr, char *buf);
-	uint8_t rank;
+	ssize_t (*show)(struct cunit_attribute *attr, char *buf);
+	uint8_t cunit;
 };
 
 struct region_attribute {
 	struct attribute attr;
 	ssize_t (*show)(struct region_attribute *attr, char *buf);
-	uint8_t rank;
+	uint8_t cunit;
 	uint8_t region;
 };
 
@@ -157,13 +157,13 @@ struct regions_sysfs {
 	struct kobject regions_kobj;
 };
 
-struct rank_sysfs {
-	struct rank_attribute attrs[REGION_SYSFS_ATTR_COUNT];
+struct cunit_sysfs {
+	struct cunit_attribute attrs[REGION_SYSFS_ATTR_COUNT];
 	struct attribute *attributes[WITH_NULL_TERM(REGION_SYSFS_ATTR_COUNT)];
 	struct attribute_group group;
 	const struct attribute_group *groups[WITH_NULL_TERM(SYSFS_GROUPS_COUNT)];
 	struct regions_sysfs regions_fs;
-	struct kobject rank_idx_kobj;
+	struct kobject cunit_idx_kobj;
 };
 
 struct raw_region_attribute {
@@ -181,54 +181,54 @@ struct raw_region_sysfs {
 	struct kobject idx_kobj;
 };
 
-static struct rank_sysfs *ranks_fs;
+static struct cunit_sysfs *cunits_fs;
 static struct raw_region_sysfs *raw_regions_fs;
 
-static const char *const rank_attr_name[] = {
+static const char *const cunit_attr_name[] = {
 	"state",
 	"size",
 	"free_size",
 	"acquisition_count",
 };
 
-static ssize_t rank_show(struct rank_attribute *attr, char *buf)
+static ssize_t cunit_show(struct cunit_attribute *attr, char *buf)
 {
 	uint8_t state;
 	uint64_t free_size, size, wr_acq_count;
 
-	if (strcmp(attr->attr.name, rank_attr_name[0]) == 0) {
-		state = rank_scheduler_rank_state(attr->rank);
+	if (strcmp(attr->attr.name, cunit_attr_name[0]) == 0) {
+		state = cunit_scheduler_cunit_state(attr->cunit);
 		return sysfs_emit(buf, "%u\n", state);
 	}
 
-	if (strcmp(attr->attr.name, rank_attr_name[1]) == 0) {
-		size = get_total_size(attr->rank);
+	if (strcmp(attr->attr.name, cunit_attr_name[1]) == 0) {
+		size = get_total_size(attr->cunit);
 		return sysfs_emit(buf, "%llu\n", size);
 	}
 
-	if (strcmp(attr->attr.name, rank_attr_name[2]) == 0) {
-		free_size = get_free_size(attr->rank);
+	if (strcmp(attr->attr.name, cunit_attr_name[2]) == 0) {
+		free_size = get_free_size(attr->cunit);
 		return sysfs_emit(buf, "%llu\n", free_size);
 	}
 
-	if (strcmp(attr->attr.name, rank_attr_name[3]) == 0) {
-		wr_acq_count = rank_scheduler_acquisition_count(attr->rank);
+	if (strcmp(attr->attr.name, cunit_attr_name[3]) == 0) {
+		wr_acq_count = cunit_scheduler_acquisition_count(attr->cunit);
 		return sysfs_emit(buf, "%llu\n", wr_acq_count);
 	}
 
 	return 0;
 }
 
-static ssize_t rank_attr_show(struct kobject *kobj, struct attribute *attr,
-			      char *buf)
+static ssize_t cunit_attr_show(struct kobject *kobj, struct attribute *attr,
+			       char *buf)
 {
-	struct rank_attribute *rank_attr =
-		container_of(attr, struct rank_attribute, attr);
+	struct cunit_attribute *cunit_attr =
+		container_of(attr, struct cunit_attribute, attr);
 
-	if (!rank_attr->show)
+	if (!cunit_attr->show)
 		return -EIO;
 
-	return rank_attr->show(rank_attr, buf);
+	return cunit_attr->show(cunit_attr, buf);
 }
 
 static const char *const region_group_name[] = {
@@ -248,7 +248,7 @@ static ssize_t region_show(struct region_attribute *attr, char *buf)
 	const size_t nr_regions_per_cunit =
 		mem_cunit_info->nr_regions / nr_cunits;
 	const struct sls_mem_cunit_region *cunit_regions =
-		&mem_cunit_info->regions[attr->rank * nr_regions_per_cunit];
+		&mem_cunit_info->regions[attr->cunit * nr_regions_per_cunit];
 	const struct sls_mem_cunit_region *region = NULL;
 	size_t idx;
 
@@ -288,7 +288,7 @@ static ssize_t region_attr_show(struct kobject *kobj, struct attribute *attr,
 	return region_attr->show(region_attr, buf);
 }
 
-static void fill_region_sysfs(struct regions_sysfs *regions_fs, uint8_t rank,
+static void fill_region_sysfs(struct regions_sysfs *regions_fs, uint8_t cunit,
 			      uint8_t region)
 {
 	int attr_counter;
@@ -300,7 +300,7 @@ static void fill_region_sysfs(struct regions_sysfs *regions_fs, uint8_t rank,
 		reg_attr->attr.name = region_attr_name[attr_counter];
 		reg_attr->attr.mode = 0444;
 		reg_attr->show = region_show;
-		reg_attr->rank = rank;
+		reg_attr->cunit = cunit;
 		reg_attr->region = region;
 
 		regions_fs->attrs[region][attr_counter] =
@@ -314,12 +314,12 @@ static void fill_region_sysfs(struct regions_sysfs *regions_fs, uint8_t rank,
 	regions_fs->groups[region] = &regions_fs->group[region];
 }
 
-static void fill_regions_sysfs(struct regions_sysfs *regions_fs, uint8_t rank)
+static void fill_regions_sysfs(struct regions_sysfs *regions_fs, uint8_t cunit)
 {
 	int region;
 
 	for (region = 0; region < SLS_BLOCK_MAX; region++)
-		fill_region_sysfs(regions_fs, rank, region);
+		fill_region_sysfs(regions_fs, cunit, region);
 
 	regions_fs->groups[SLS_BLOCK_MAX] = NULL;
 }
@@ -328,14 +328,14 @@ static const struct sysfs_ops regions_sysfs_ops = {
 	.show = region_attr_show,
 };
 
-/* Operations on memory regions inside rank */
+/* Operations on memory regions inside cunit */
 static struct kobj_type regions_type = {
 	.sysfs_ops = &regions_sysfs_ops,
 };
 
-static int build_rank_regions_sysfs(struct kobject *kobj,
-				    struct regions_sysfs *regions_fs,
-				    uint8_t rank)
+static int build_cunit_regions_sysfs(struct kobject *kobj,
+				     struct regions_sysfs *regions_fs,
+				     uint8_t cunit)
 {
 	int err;
 
@@ -345,82 +345,83 @@ static int build_rank_regions_sysfs(struct kobject *kobj,
 		kobject_put(&regions_fs->regions_kobj);
 		memset(&regions_fs->regions_kobj, 0,
 		       sizeof(regions_fs->regions_kobj));
-		goto rank_regions_out;
+		goto cunit_regions_out;
 	}
 
-	fill_regions_sysfs(regions_fs, rank);
+	fill_regions_sysfs(regions_fs, cunit);
 
 	err = sysfs_create_groups(&regions_fs->regions_kobj,
 				  regions_fs->groups);
 	if (err)
-		goto rank_regions_out;
+		goto cunit_regions_out;
 
-rank_regions_out:
+cunit_regions_out:
 	return err;
 }
 
-static void fill_rank_attrs(struct rank_sysfs *rank_fs, uint8_t rank)
+static void fill_cunit_attrs(struct cunit_sysfs *cunit_fs, uint8_t cunit)
 {
 	int attr_idx;
 
 	for (attr_idx = 0; attr_idx < REGION_SYSFS_ATTR_COUNT; ++attr_idx) {
-		rank_fs->attrs[attr_idx].attr.name = rank_attr_name[attr_idx];
-		rank_fs->attrs[attr_idx].attr.mode = 0444;
-		rank_fs->attrs[attr_idx].show = rank_show;
-		rank_fs->attrs[attr_idx].rank = rank;
+		cunit_fs->attrs[attr_idx].attr.name = cunit_attr_name[attr_idx];
+		cunit_fs->attrs[attr_idx].attr.mode = 0444;
+		cunit_fs->attrs[attr_idx].show = cunit_show;
+		cunit_fs->attrs[attr_idx].cunit = cunit;
 
-		rank_fs->attributes[attr_idx] = &rank_fs->attrs[attr_idx].attr;
+		cunit_fs->attributes[attr_idx] =
+			&cunit_fs->attrs[attr_idx].attr;
 	}
 
-	rank_fs->attributes[REGION_SYSFS_ATTR_COUNT] = NULL;
+	cunit_fs->attributes[REGION_SYSFS_ATTR_COUNT] = NULL;
 }
 
-static void fill_rank_sysfs(struct rank_sysfs *rank_fs, uint8_t rank)
+static void fill_cunit_sysfs(struct cunit_sysfs *cunit_fs, uint8_t cunit)
 {
-	fill_rank_attrs(rank_fs, rank);
-	rank_fs->group.attrs = rank_fs->attributes;
-	rank_fs->groups[0] = &rank_fs->group;
-	rank_fs->groups[1] = NULL;
+	fill_cunit_attrs(cunit_fs, cunit);
+	cunit_fs->group.attrs = cunit_fs->attributes;
+	cunit_fs->groups[0] = &cunit_fs->group;
+	cunit_fs->groups[1] = NULL;
 }
 
-static const struct sysfs_ops rank_sysfs_ops = {
-	.show = rank_attr_show,
+static const struct sysfs_ops cunit_sysfs_ops = {
+	.show = cunit_attr_show,
 };
 
-/* Operations on rank stats */
-static const struct kobj_type rank_type = {
-	.sysfs_ops = &rank_sysfs_ops,
+/* Operations on cunit stats */
+static const struct kobj_type cunit_type = {
+	.sysfs_ops = &cunit_sysfs_ops,
 };
 
-static int build_rank_sysfs(struct kobject *kobj, uint8_t rank)
+static int build_cunit_sysfs(struct kobject *kobj, uint8_t cunit)
 {
 	char buf[4];
 	int err;
 
-	SLS_DBG("Building SLS sysfs for rank %hhu\n", rank);
+	SLS_DBG("Building SLS sysfs for cunit %hhu\n", cunit);
 
-	kobject_init(&ranks_fs[rank].rank_idx_kobj, &rank_type);
-	sprintf(buf, "%hhu", rank);
-	err = kobject_add(&ranks_fs[rank].rank_idx_kobj, kobj, buf);
+	kobject_init(&cunits_fs[cunit].cunit_idx_kobj, &cunit_type);
+	sprintf(buf, "%hhu", cunit);
+	err = kobject_add(&cunits_fs[cunit].cunit_idx_kobj, kobj, buf);
 	if (err) {
-		kobject_put(&ranks_fs[rank].rank_idx_kobj);
-		memset(&ranks_fs[rank].rank_idx_kobj, 0,
+		kobject_put(&cunits_fs[cunit].cunit_idx_kobj);
+		memset(&cunits_fs[cunit].cunit_idx_kobj, 0,
 		       sizeof(struct kobject));
-		goto build_rank_out;
+		goto build_cunit_out;
 	}
 
-	fill_rank_sysfs(&ranks_fs[rank], rank);
+	fill_cunit_sysfs(&cunits_fs[cunit], cunit);
 
-	if (sysfs_create_groups(&ranks_fs[rank].rank_idx_kobj,
-				ranks_fs[rank].groups)) {
+	if (sysfs_create_groups(&cunits_fs[cunit].cunit_idx_kobj,
+				cunits_fs[cunit].groups)) {
 		err = -ENOMEM;
-		goto build_rank_out;
+		goto build_cunit_out;
 	}
 
-	err = build_rank_regions_sysfs(&ranks_fs[rank].rank_idx_kobj,
-				       &ranks_fs[rank].regions_fs, rank);
+	err = build_cunit_regions_sysfs(&cunits_fs[cunit].cunit_idx_kobj,
+					&cunits_fs[cunit].regions_fs, cunit);
 
-build_rank_out:
+build_cunit_out:
 	return err;
 }
 
@@ -626,22 +627,22 @@ int build_sls_sysfs(const struct sls_mem_cunit_info *mem_cunitinfo,
 		    const struct sls_mem_info *meminfo,
 		    struct device *resource_dev)
 {
-	int rank;
+	int cunit;
 	int err = 0;
 
 	mem_cunit_info = mem_cunitinfo;
 	mem_info = meminfo;
 
 	SLS_DBG("Building SLS sysfs\n");
-	ranks_kobj =
-		kobject_create_and_add(DEVICE_RANKS_PATH, &resource_dev->kobj);
+	cunits_kobj =
+		kobject_create_and_add(DEVICE_CUNITS_PATH, &resource_dev->kobj);
 
-	if (!ranks_kobj) {
+	if (!cunits_kobj) {
 		SLS_ERR("Failed to create cunits kobject\n");
 		return -ENOMEM;
 	}
 
-	err = sysfs_create_groups(ranks_kobj, dev_attr_groups);
+	err = sysfs_create_groups(cunits_kobj, dev_attr_groups);
 	if (err) {
 		SLS_ERR("Failed to create sysfs groups\n");
 		goto cunits_kobj_free;
@@ -659,24 +660,26 @@ int build_sls_sysfs(const struct sls_mem_cunit_info *mem_cunitinfo,
 		goto mappings_free;
 	}
 
-	ranks_fs = kcalloc(sls_topo()->nr_cunits, sizeof(struct rank_sysfs),
-			   GFP_KERNEL);
-	if (!ranks_fs) {
+	cunits_fs = kcalloc(sls_topo()->nr_cunits, sizeof(struct cunit_sysfs),
+			    GFP_KERNEL);
+	if (!cunits_fs) {
 		SLS_ERR("No free memory for cunits directories\n");
 		goto topology_free;
 	}
 
-	for (rank = 0; rank < sls_topo()->nr_cunits; ++rank) {
-		err = build_rank_sysfs(ranks_kobj, rank);
+	for (cunit = 0; cunit < sls_topo()->nr_cunits; ++cunit) {
+		err = build_cunit_sysfs(cunits_kobj, cunit);
 		if (err) {
-			SLS_ERR("Failed to build sysfs for cunit [%d]\n", rank);
-			while (--rank >= 0) {
+			SLS_ERR("Failed to build sysfs for cunit [%d]\n",
+				cunit);
+			while (--cunit >= 0) {
 				sysfs_remove_groups(
-					&ranks_fs[rank].regions_fs.regions_kobj,
-					ranks_fs[rank].regions_fs.groups);
+					&cunits_fs[cunit]
+						 .regions_fs.regions_kobj,
+					cunits_fs[cunit].regions_fs.groups);
 				sysfs_remove_groups(
-					&ranks_fs[rank].rank_idx_kobj,
-					ranks_fs[rank].groups);
+					&cunits_fs[cunit].cunit_idx_kobj,
+					cunits_fs[cunit].groups);
 			}
 			goto fs_free;
 		}
@@ -686,36 +689,36 @@ int build_sls_sysfs(const struct sls_mem_cunit_info *mem_cunitinfo,
 	return err;
 
 fs_free:
-	kfree(ranks_fs);
+	kfree(cunits_fs);
 topology_free:
 	sls_destroy_topology_constants();
 mappings_free:
 	sls_destroy_mappings();
 groups_free:
-	sysfs_remove_groups(ranks_kobj, dev_attr_groups);
+	sysfs_remove_groups(cunits_kobj, dev_attr_groups);
 cunits_kobj_free:
-	kobject_del(ranks_kobj);
+	kobject_del(cunits_kobj);
 	return err ? err : -ENOMEM;
 }
 
 void destroy_sls_sysfs(void)
 {
-	int rank;
+	int cunit;
 
 	SLS_DBG("Destroying SLS sysfs\n");
 
-	for (rank = 0; rank < sls_topo()->nr_cunits; ++rank) {
-		sysfs_remove_groups(&ranks_fs[rank].regions_fs.regions_kobj,
-				    ranks_fs[rank].regions_fs.groups);
-		kobject_del(&ranks_fs[rank].regions_fs.regions_kobj);
-		sysfs_remove_groups(ranks_kobj, ranks_fs[rank].groups);
-		kobject_del(&ranks_fs[rank].rank_idx_kobj);
+	for (cunit = 0; cunit < sls_topo()->nr_cunits; ++cunit) {
+		sysfs_remove_groups(&cunits_fs[cunit].regions_fs.regions_kobj,
+				    cunits_fs[cunit].regions_fs.groups);
+		kobject_del(&cunits_fs[cunit].regions_fs.regions_kobj);
+		sysfs_remove_groups(cunits_kobj, cunits_fs[cunit].groups);
+		kobject_del(&cunits_fs[cunit].cunit_idx_kobj);
 	}
 
 	sls_destroy_topology_constants();
-	sysfs_remove_groups(ranks_kobj, dev_attr_groups);
-	kobject_del(ranks_kobj);
-	kfree(ranks_fs);
+	sysfs_remove_groups(cunits_kobj, dev_attr_groups);
+	kobject_del(cunits_kobj);
+	kfree(cunits_fs);
 
 	SLS_DBG("Destroyed SLS sysfs\n");
 }
diff --git a/include/uapi/linux/sls_resources.h b/include/uapi/linux/sls_resources.h
index 85c5a2554..555afabcb 100644
--- a/include/uapi/linux/sls_resources.h
+++ b/include/uapi/linux/sls_resources.h
@@ -40,7 +40,7 @@
 #define SLS_SYSFS_ROOT "/sys/class/" SLS_RESOURCE_PATH_INTERNAL
 
 /* Path for cunits info */
-#define DEVICE_RANKS_PATH "ranks"
+#define DEVICE_CUNITS_PATH "cunits"
 /* Path for mappings info */
 #define DEVICE_MAPPINGS_PATH "mappings"
 
@@ -111,45 +111,45 @@
 #define ATTR_INST_BUFFER_PATH DEVICE_TOPOLOGY_CONSTANT_PATH(nr_inst_buf)
 
 // Block of sysfs paths for cunits info, all O_RDONLY
-// SLS_RESOURCE_PATH/ranks/%d
+// SLS_RESOURCE_PATH/cunit/%d
 
 /* O_RDONLY Rank state, 0 = free, 1 = busy */
-#define RANK_STATE_PATH "state"
+#define CUNIT_STATE_PATH "state"
 /* O_RDWR Rank acquisitions count */
-#define RANK_ACQUISITION_COUNT_PATH "acquisition_count"
+#define CUNIT_ACQUISITION_COUNT_PATH "acquisition_count"
 /* O_RDONLY Get free size in bytes */
-#define RANK_FREE_SIZE_PATH "free_size"
+#define CUNIT_FREE_SIZE_PATH "free_size"
 
-#define RANK_REGION_SIZE_PATH(region) "regions/" #region "/size"
-#define RANK_REGION_OFFSET_PATH(region) "regions/" #region "/offset"
-#define RANK_REGION_MAP_SIZE_PATH(region) "regions/" #region "/map_size"
-#define RANK_REGION_MAP_OFFSET_PATH(region) "regions/" #region "/map_offset"
+#define CUNIT_REGION_SIZE_PATH(region) "regions/" #region "/size"
+#define CUNIT_REGION_OFFSET_PATH(region) "regions/" #region "/offset"
+#define CUNIT_REGION_MAP_SIZE_PATH(region) "regions/" #region "/map_size"
+#define CUNIT_REGION_MAP_OFFSET_PATH(region) "regions/" #region "/map_offset"
 
 /* BASE region's size and offset */
-#define RANK_REGION_BASE_SIZE_PATH RANK_REGION_SIZE_PATH(base)
-#define RANK_REGION_BASE_OFFSET_PATH RANK_REGION_OFFSET_PATH(base)
-#define RANK_REGION_BASE_MAP_SIZE_PATH RANK_REGION_MAP_SIZE_PATH(base)
-#define RANK_REGION_BASE_MAP_OFFSET_PATH RANK_REGION_MAP_OFFSET_PATH(base)
+#define CUNIT_REGION_BASE_SIZE_PATH CUNIT_REGION_SIZE_PATH(base)
+#define CUNIT_REGION_BASE_OFFSET_PATH CUNIT_REGION_OFFSET_PATH(base)
+#define CUNIT_REGION_BASE_MAP_SIZE_PATH CUNIT_REGION_MAP_SIZE_PATH(base)
+#define CUNIT_REGION_BASE_MAP_OFFSET_PATH CUNIT_REGION_MAP_OFFSET_PATH(base)
 /* INST region's size and offset */
-#define RANK_REGION_INST_SIZE_PATH RANK_REGION_SIZE_PATH(inst)
-#define RANK_REGION_INST_OFFSET_PATH RANK_REGION_OFFSET_PATH(inst)
-#define RANK_REGION_INST_MAP_SIZE_PATH RANK_REGION_MAP_SIZE_PATH(inst)
-#define RANK_REGION_INST_MAP_OFFSET_PATH RANK_REGION_MAP_OFFSET_PATH(inst)
+#define CUNIT_REGION_INST_SIZE_PATH CUNIT_REGION_SIZE_PATH(inst)
+#define CUNIT_REGION_INST_OFFSET_PATH CUNIT_REGION_OFFSET_PATH(inst)
+#define CUNIT_REGION_INST_MAP_SIZE_PATH CUNIT_REGION_MAP_SIZE_PATH(inst)
+#define CUNIT_REGION_INST_MAP_OFFSET_PATH CUNIT_REGION_MAP_OFFSET_PATH(inst)
 /* CFGR region's size and offset */
-#define RANK_REGION_CFGR_SIZE_PATH RANK_REGION_SIZE_PATH(cfgr)
-#define RANK_REGION_CFGR_OFFSET_PATH RANK_REGION_OFFSET_PATH(cfgr)
-#define RANK_REGION_CFGR_MAP_SIZE_PATH RANK_REGION_MAP_SIZE_PATH(cfgr)
-#define RANK_REGION_CFGR_MAP_OFFSET_PATH RANK_REGION_MAP_OFFSET_PATH(cfgr)
+#define CUNIT_REGION_CFGR_SIZE_PATH CUNIT_REGION_SIZE_PATH(cfgr)
+#define CUNIT_REGION_CFGR_OFFSET_PATH CUNIT_REGION_OFFSET_PATH(cfgr)
+#define CUNIT_REGION_CFGR_MAP_SIZE_PATH CUNIT_REGION_MAP_SIZE_PATH(cfgr)
+#define CUNIT_REGION_CFGR_MAP_OFFSET_PATH CUNIT_REGION_MAP_OFFSET_PATH(cfgr)
 /* TAGS region's size and offset */
-#define RANK_REGION_TAGS_SIZE_PATH RANK_REGION_SIZE_PATH(tags)
-#define RANK_REGION_TAGS_OFFSET_PATH RANK_REGION_OFFSET_PATH(tags)
-#define RANK_REGION_TAGS_MAP_SIZE_PATH RANK_REGION_MAP_SIZE_PATH(tags)
-#define RANK_REGION_TAGS_MAP_OFFSET_PATH RANK_REGION_MAP_OFFSET_PATH(tags)
+#define CUNIT_REGION_TAGS_SIZE_PATH CUNIT_REGION_SIZE_PATH(tags)
+#define CUNIT_REGION_TAGS_OFFSET_PATH CUNIT_REGION_OFFSET_PATH(tags)
+#define CUNIT_REGION_TAGS_MAP_SIZE_PATH CUNIT_REGION_MAP_SIZE_PATH(tags)
+#define CUNIT_REGION_TAGS_MAP_OFFSET_PATH CUNIT_REGION_MAP_OFFSET_PATH(tags)
 /* PSUM region's size and offset */
-#define RANK_REGION_PSUM_SIZE_PATH RANK_REGION_SIZE_PATH(psum)
-#define RANK_REGION_PSUM_OFFSET_PATH RANK_REGION_OFFSET_PATH(psum)
-#define RANK_REGION_PSUM_MAP_SIZE_PATH RANK_REGION_MAP_SIZE_PATH(psum)
-#define RANK_REGION_PSUM_MAP_OFFSET_PATH RANK_REGION_MAP_OFFSET_PATH(psum)
+#define CUNIT_REGION_PSUM_SIZE_PATH CUNIT_REGION_SIZE_PATH(psum)
+#define CUNIT_REGION_PSUM_OFFSET_PATH CUNIT_REGION_OFFSET_PATH(psum)
+#define CUNIT_REGION_PSUM_MAP_SIZE_PATH CUNIT_REGION_MAP_SIZE_PATH(psum)
+#define CUNIT_REGION_PSUM_MAP_OFFSET_PATH CUNIT_REGION_MAP_OFFSET_PATH(psum)
 
 /* The enumeration of sls blocks addresses */
 enum sls_mem_blocks_e {
@@ -172,25 +172,25 @@ enum sls_user_preferences {
 };
 
 #define SLS_USER_PREF_BITS 8
-#define SLS_USER_RANK_BITS 8
+#define SLS_USER_CUNIT_BITS 8
 
 #define SLS_USER_PREF_MASK ((1U << SLS_USER_PREF_BITS) - 1)
-#define SLS_USER_RANK_MASK \
-	(((1U << SLS_USER_RANK_BITS) - 1) << SLS_USER_PREF_BITS)
+#define SLS_USER_CUNIT_MASK \
+	(((1U << SLS_USER_CUNIT_BITS) - 1) << SLS_USER_PREF_BITS)
 
-#define SLS_ALLOC_SINGLE_RANK(rank) \
+#define SLS_ALLOC_SINGLE_CUNIT(rank) \
 	(SLS_ALLOC_SINGLE | ((rank + 1) << SLS_USER_PREF_BITS))
 
 #define GET_ALLOC_POLICY(preference) (preference & SLS_USER_PREF_MASK)
-#define GET_ALLOC_SINGLE_RANK_PREFERENCE(preference) \
-	((preference & SLS_USER_RANK_MASK) >> SLS_USER_PREF_BITS)
+#define GET_ALLOC_SINGLE_CUNIT_PREFERENCE(preference) \
+	((preference & SLS_USER_CUNIT_MASK) >> SLS_USER_PREF_BITS)
 
-#define SLS_ALLOC_ANY_RANK 0xffffffffU
+#define SLS_ALLOC_ANY_CUNIT 0xffffffffU
 
 #pragma pack(push, 1)
 struct sls_memory_alloc_request {
-	uint32_t rank;
-	uint64_t rank_offset;
+	uint32_t cunit;
+	uint64_t cunit_offset;
 	uint64_t size;
 };
 #pragma pack(pop)
@@ -201,10 +201,10 @@ struct sls_memory_alloc_request {
 #define SET_FIRST_BUFFER _IO(SLS_IOC_MAGIC, 1)
 #define SET_SECOND_BUFFER _IO(SLS_IOC_MAGIC, 2)
 #define WHICH_BUFFER _IOR(SLS_IOC_MAGIC, 3, int)
-#define GET_RANK_FOR_WRITE _IOW(SLS_IOC_MAGIC, 4, unsigned int)
-#define GET_RANK_FOR_READ _IOW(SLS_IOC_MAGIC, 5, unsigned int)
-#define RELEASE_WRITE_RANK _IOW(SLS_IOC_MAGIC, 6, unsigned int)
-#define RELEASE_READ_RANK _IOW(SLS_IOC_MAGIC, 7, unsigned int)
+#define GET_CUNIT_FOR_WRITE _IOW(SLS_IOC_MAGIC, 4, unsigned int)
+#define GET_CUNIT_FOR_READ _IOW(SLS_IOC_MAGIC, 5, unsigned int)
+#define RELEASE_WRITE_CUNIT _IOW(SLS_IOC_MAGIC, 6, unsigned int)
+#define RELEASE_READ_CUNIT _IOW(SLS_IOC_MAGIC, 7, unsigned int)
 #define ALLOCATE_MEMORY _IOWR(SLS_IOC_MAGIC, 8, struct sls_memory_alloc_request)
 #define DEALLOCATE_MEMORY \
 	_IOW(SLS_IOC_MAGIC, 9, struct sls_memory_alloc_request)
-- 
2.34.1

