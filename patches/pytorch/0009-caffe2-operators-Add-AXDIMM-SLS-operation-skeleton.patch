From 3db7eb7c395308c309a7756d4ce8ee1989a6abed Mon Sep 17 00:00:00 2001
From: Maxim Ostapenko <maxim.o@samsung.com>
Date: Mon, 1 Nov 2021 16:58:14 +0900
Subject: [PATCH 009/135] [caffe2/operators] Add AXDIMM SLS operation skeleton

Add necessary cmake support + add necessary dispatching skeleton for AXDIMM
mapped SparseLengthsSum operation. The implementation is still incomplete but
now just requires linking against libaxdimm.so and calling axd_run function
with proper parameters.

Signed-off-by: Maxim Ostapenko <maxim.o@samsung.com>
---
 caffe2/CMakeLists.txt                         |   4 +
 caffe2/operators/lengths_reducer_ops.h        |   1 +
 .../operators/lengths_reducer_ops_axdimm.cc   | 122 ++++++++++++++++++
 3 files changed, 127 insertions(+)

diff --git a/caffe2/CMakeLists.txt b/caffe2/CMakeLists.txt
index 79418170..184cee5b 100644
--- a/caffe2/CMakeLists.txt
+++ b/caffe2/CMakeLists.txt
@@ -999,6 +999,7 @@ if(NOT INTERN_BUILD_MOBILE)
   # to establish the dependency, since the generation procedure is declared in a different CMake file.
   # See https://samthursfield.wordpress.com/2015/11/21/cmake-dependencies-between-targets-and-files-and-custom-commands/#custom-commands-in-different-directories
   add_dependencies(torch_cpu Caffe2_PROTO)
+  add_dependencies(torch_axdimm Caffe2_PROTO)
 endif()
 
 # Build model tracer for tracing-based selective build
@@ -1308,10 +1309,13 @@ endif()
 if(NOT INTERN_BUILD_MOBILE)
   caffe2_interface_library(caffe2_protos caffe2_protos_whole)
   target_link_libraries(torch_cpu PRIVATE caffe2_protos_whole)
+  target_link_libraries(torch_axdimm PRIVATE caffe2_protos_whole)
   if(${CAFFE2_LINK_LOCAL_PROTOBUF})
     target_link_libraries(torch_cpu INTERFACE protobuf::libprotobuf)
+    target_link_libraries(torch_axdimm INTERFACE protobuf::libprotobuf)
   else()
     target_link_libraries(torch_cpu PUBLIC protobuf::libprotobuf)
+    target_link_libraries(torch_axdimm PUBLIC protobuf::libprotobuf)
   endif()
 endif()
 
diff --git a/caffe2/operators/lengths_reducer_ops.h b/caffe2/operators/lengths_reducer_ops.h
index 83199db8..49ba3963 100644
--- a/caffe2/operators/lengths_reducer_ops.h
+++ b/caffe2/operators/lengths_reducer_ops.h
@@ -2,6 +2,7 @@
 
 #include <c10/util/irange.h>
 #include "caffe2/core/context.h"
+#include "caffe2/core/context_axdimm.h"
 #include "caffe2/core/operator.h"
 #include "caffe2/perfkernels/embedding_lookup.h"
 #ifdef USE_FBGEMM
diff --git a/caffe2/operators/lengths_reducer_ops_axdimm.cc b/caffe2/operators/lengths_reducer_ops_axdimm.cc
index e69de29b..4701182a 100644
--- a/caffe2/operators/lengths_reducer_ops_axdimm.cc
+++ b/caffe2/operators/lengths_reducer_ops_axdimm.cc
@@ -0,0 +1,122 @@
+#undef USE_FBGEMM
+#include "caffe2/operators/lengths_reducer_ops.h"
+#include "caffe2/core/context.h"
+#include "caffe2/core/operator.h"
+#include "caffe2/operators/segment_reduction_op.h"
+#include "caffe2/utils/math.h"
+
+namespace caffe2 {
+
+// A templated class that implements SparseLengths[Sum,WeightedSum,Mean] for AXDIMM.
+template <
+    typename T, // output type
+    class InputTypes, // supported input types, such as TensorTypes<float>
+    bool USE_WEIGHT = false, // Whether it is SparseLengthsWeightedSum
+    bool USE_MEAN = false, // Whether this is SparseLengthsMean
+    bool USE_POSITIONAL_WEIGHT = false
+    // USE_WEIGHT = true and USE_POSITIONAL_WEIGHT = true
+    // -> SparseLengthsPositionalWeightedSum
+    >
+class AXDIMMSparseLengthsReductionOp : public Operator<AXDIMMContext> {
+ public:
+  USE_OPERATOR_FUNCTIONS(AXDIMMContext);
+  template <class... Args>
+  explicit AXDIMMSparseLengthsReductionOp(Args&&... args)
+      : Operator<AXDIMMContext>(std::forward<Args>(args)...) {
+    static_assert(
+        !(USE_WEIGHT & USE_MEAN), "Cannot both specify weight and mean.");
+  }
+
+  ~AXDIMMSparseLengthsReductionOp() {}
+
+  // Currently, we support float and at::Half inputs for input data type, and
+  // int32_t and int64_t for the index type.
+
+  bool RunOnDevice() override {
+    return DispatchHelper<InputTypes>::call(this, Input(DATA));
+  }
+
+  template <typename InputType>
+  bool DoRunWithType() {
+    return DispatchHelper<TensorTypes2<int32_t, int64_t>, InputType>::call(
+        this, Input(INDICES));
+  }
+
+  template <typename InputType, typename IndexType>
+  bool DoRunWithType2() {
+    auto& dataInput = Input(DATA);
+    auto& indicesInput = Input(INDICES);
+    auto& lengthsInput = Input(LENGTHS);
+
+    const int64_t M = lengthsInput.size(0);
+    const int64_t indices_size = indicesInput.numel();
+
+    auto shape = dataInput.sizes().vec();
+    shape[0] = M;
+    auto* output = Output(0, shape, at::dtype<T>());
+    T* out_data = output->template mutable_data<T>();
+
+    if (indices_size == 0) {
+      if (M > 0) {
+        memset(out_data, 0, output->numel() * sizeof(T));
+      }
+      return true;
+    }
+
+    CAFFE_ENFORCE_EQ(1, indicesInput.dim(), "INDICES must be a vector");
+    CAFFE_ENFORCE_EQ(1, lengthsInput.dim(), "LENGTHS must be a vector");
+    const int64_t N = dataInput.size(0);
+    const int D = dataInput.size_from_dim(1);
+
+    const InputType* in_data = dataInput.template data<InputType>();
+    const IndexType* indices = indicesInput.template data<IndexType>();
+    const int* lengths = lengthsInput.template data<int>();
+    const T* in_weight = nullptr;
+
+    if (USE_WEIGHT) {
+      // static if
+      auto& weightInput = Input(WEIGHT);
+      CAFFE_ENFORCE_EQ(1, weightInput.dim(), "WEIGHT must be a vector");
+      if (!USE_POSITIONAL_WEIGHT) {
+        CAFFE_ENFORCE_EQ(
+            weightInput.numel(),
+            indices_size,
+            "Weight should have the same length as indices.");
+      }
+      in_weight = weightInput.template data<T>();
+    }
+
+    // TODO: implement actual run here.
+    // // delegate work to perfkernel that branches based on architecture
+    // EmbeddingLookup<IndexType, InputType, T, USE_POSITIONAL_WEIGHT>(
+    //     D,
+    //     M,
+    //     indices_size,
+    //     N,
+    //     in_data,
+    //     indices,
+    //     lengths,
+    //     in_weight,
+    //     nullptr, // scale_bias field is only used in SparseLengths8BitsRowwiseOp
+    //     USE_MEAN,
+    //     out_data);
+    return true;
+  }
+
+  enum {
+    DATA = 0, // Data input.
+    WEIGHT = 1, // Weight input used in SparseLengthsWeightedSum
+    INDICES = 1 + USE_WEIGHT, // 1 in SparseLengths[Sum,Mean] and
+                              // 2 in SparseLengthsWeightedSum
+    LENGTHS = 2 + USE_WEIGHT, // 2 in SparseLengths[Sum, Mean],
+                              // 3 in SparseLengthsWeightedSum
+  };
+};
+
+
+using SparseLengthsSumOp =
+    // NOLINTNEXTLINE(modernize-use-bool-literals)
+    AXDIMMSparseLengthsReductionOp<float, TensorTypes<float, at::Half>, 0, 0>;
+
+REGISTER_AXDIMM_OPERATOR(SparseLengthsSum, SparseLengthsSumOp);
+} // namespace caffe2
-- 
2.34.1

