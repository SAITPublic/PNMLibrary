From b959597f9d0a64578f24b95b705cd5cf4c42dadc Mon Sep 17 00:00:00 2001
From: Maxim Ostapenko <maxim.o@samsung.com>
Date: Tue, 9 May 2023 11:11:10 +0000
Subject: [PATCH 21/23] Rename AXDIMM -> PNM to reflect pytorch changes

* Perform massive AXDIMM -> PNM renaming to reflect respective pytorch
  changes and generalize DLRM usage.
* Introduce "--use_pnm" flag to run_DeepRecSys_pnm.sh.
* Update README.md.

Signed-off-by: Maxim Ostapenko <maxim.o@samsung.com>
---
 axdimm_utils.py                     | 46 -----------------
 dlrm_s_pytorch.py                   | 76 ++++++++++++++---------------
 pnm_utils.py                        | 46 +++++++++++++++++
 run_test.sh                         |  8 +--
 test_axdimm_op.py => test_pnm_op.py | 46 ++++++++---------
 5 files changed, 111 insertions(+), 111 deletions(-)
 delete mode 100644 axdimm_utils.py
 create mode 100644 pnm_utils.py
 rename test_axdimm_op.py => test_pnm_op.py (85%)

diff --git a/axdimm_utils.py b/axdimm_utils.py
deleted file mode 100644
index 2eeebfb..0000000
--- a/axdimm_utils.py
+++ /dev/null
@@ -1,46 +0,0 @@
-import torch
-
-import enum
-
-class AxdimmTensorType(enum.Enum):
-    simple = enum.auto()
-    secure = enum.auto()
-    secure_tagged = enum.auto()
-    
-    def __str__(self):
-        return self.name
-
-to_internal_type = {
-    AxdimmTensorType.simple.name: AxdimmTensorType.simple.name,
-    AxdimmTensorType.simple.secure.name: AxdimmTensorType.simple.secure.name,
-    AxdimmTensorType.simple.secure_tagged.name: AxdimmTensorType.simple.secure.name,
-}
-    
-class AxdimmAllocPreference(enum.Enum):
-    auto = 0
-    replicate = 1
-    distribute = 2
-
-    def __str__(self):
-        return self.name
-
-def get_enum(enum_class, name):
-    val = getattr(enum_class, str(name), None)
-    if val is None:
-        raise ValueError("Failed to convert {} to enum of type {}".format(name, enum_class.__name__))
-
-    return val
-
-def create_axdimm_tensor_from_file(tensor_type, path, scalar_type, rows, num_tables, sp_size, alloc_pref = AxdimmAllocPreference.distribute):
-    with_tag = True if "tagged" in str(tensor_type) else False
-    tensor_type = to_internal_type.get(get_enum(AxdimmTensorType, tensor_type).name)
-    alloc_pref_val = get_enum(AxdimmAllocPreference, alloc_pref).value
-
-    return torch.ops.axdimm.create_tensor(tensor_type, path, scalar_type, rows, num_tables, sp_size, alloc_pref_val, with_tag)
-
-def create_axdimm_tensor(tensor_type, weights, alloc_pref = AxdimmAllocPreference.distribute):
-    with_tag = True if "tagged" in str(tensor_type) else False
-    tensor_type = to_internal_type.get(get_enum(AxdimmTensorType, tensor_type).name)
-    alloc_pref_val = get_enum(AxdimmAllocPreference, alloc_pref).value
-
-    return torch.ops.axdimm.create_tensor(tensor_type, weights, weights.size(0), weights.size(1), weights.size(2), alloc_pref_val, with_tag)
diff --git a/dlrm_s_pytorch.py b/dlrm_s_pytorch.py
index 90fc084..7f7c22b 100644
--- a/dlrm_s_pytorch.py
+++ b/dlrm_s_pytorch.py
@@ -72,7 +72,7 @@ import warnings
 # data generation
 import dlrm_data_pytorch as dp
 
-from axdimm_utils import *
+from pnm_utils import *
 
 # For distributed run
 import extend_distributed as ext_dist
@@ -94,7 +94,7 @@ from torch.nn.parameter import Parameter
 from torch.optim.lr_scheduler import _LRScheduler
 import optim.rwsadagrad as RowWiseSparseAdagrad
 from torch.utils.tensorboard import SummaryWriter
-from torch.nn import EmbeddingBagAxdimm
+from torch.nn import EmbeddingBagPnm
 
 # mixed-dimension trick
 from tricks.md_embedding_bag import PrEmbeddingBag, md_solver
@@ -238,11 +238,11 @@ class DLRM_Net(nn.Module):
         # approach 2: use Sequential container to wrap all layers
         return torch.nn.Sequential(*layers)
 
-    def create_axdimm_emb(self, m, ln):
-        print("Creating AXDIMM tensor of type:", self.axdimm_tensor_type)
+    def create_pnm_emb(self, m, ln):
+        print("Creating PNM tensor of type:", self.pnm_tensor_type)
         from_file = self.weights_path is not None
         print("Load tensor data from file is {}".format("enable" if from_file else "disable"))
-        axdimm_weights = []
+        pnm_weights = []
         v_W_l = []
         rows = []
         for i in range(0, ln.size):
@@ -261,21 +261,21 @@ class DLRM_Net(nn.Module):
                 # 1st dim - tables number
                 # 2nd dim - features number per table
                 # 3rd dim - sparse feature size
-                axdimm_weights.append(W)
+                pnm_weights.append(W)
 
         if from_file:
-            op = EmbeddingBagAxdimm(create_axdimm_tensor_from_file(self.axdimm_tensor_type, self.weights_path, torch.float32,
-                                                                   torch.tensor(rows, dtype=torch.int32), len(rows), m))
+            op = EmbeddingBagPnm(create_pnm_tensor_from_file(self.pnm_tensor_type, self.weights_path, torch.float32,
+                                                             torch.tensor(rows, dtype=torch.int32), len(rows), m))
         else:
-            torch_ten = torch.from_numpy(np.asarray(axdimm_weights))
-            op = EmbeddingBagAxdimm(create_axdimm_tensor(self.axdimm_tensor_type, torch_ten))
+            torch_ten = torch.from_numpy(np.asarray(pnm_weights))
+            op = EmbeddingBagPnm(create_pnm_tensor(self.pnm_tensor_type, torch_ten))
         return op, v_W_l
 
     def create_emb(self, m, ln, weighted_pooling=None):
-        if self.axdimm_tensor_type is not None:
+        if self.pnm_tensor_type is not None:
             if weighted_pooling:
-                raise RuntimeError("Weighted pooling is not suported with AXDIMM device")
-            return self.create_axdimm_emb(m, ln)
+                raise RuntimeError("Weighted pooling is not suported with PNM device")
+            return self.create_pnm_emb(m, ln)
 
         emb_l = nn.ModuleList()
         v_W_l = []
@@ -345,7 +345,7 @@ class DLRM_Net(nn.Module):
         md_threshold=200,
         weighted_pooling=None,
         loss_function="bce",
-        axdimm_tensor_type=None,
+        pnm_tensor_type=None,
         weights_path=None
     ):
         super(DLRM_Net, self).__init__()
@@ -359,7 +359,7 @@ class DLRM_Net(nn.Module):
         ):
 
             # save arguments
-            self.axdimm_tensor_type = axdimm_tensor_type
+            self.pnm_tensor_type = pnm_tensor_type
             self.weights_path = weights_path
             self.ndevices = ndevices
             self.output_d = 0
@@ -449,16 +449,16 @@ class DLRM_Net(nn.Module):
         # 3. for a list of embedding tables there is a list of batched lookups
 
         ly = []
-        if self.axdimm_tensor_type is not None:
-            # Run Axdimm SLS
-            axdimm_res = emb_l(lS_i, lS_o)
+        if self.pnm_tensor_type is not None:
+            # Run PNM SLS
+            pnm_res = emb_l(lS_i, lS_o)
             # offsets are repsresented like set of distances i.e. [0, 20, 60, 90] -> [20, 40, 30] - lenghts
             # amount of elements in one offsets batch is equal to minibatch size + 1, since table indices size was appended to offsets
             # this way there's no need to pass total indices lenghts separately
-            # Axdimm returns 1D tensor and dlrm expects it to be list of tensors where each tensor is 1 table batches result
+            # PNM returns 1D tensor and dlrm expects it to be list of tensors where each tensor is 1 table batches result
             # Result is 3d tensor of dims: (len(weights), minibatch_size, sparse_feature_size)
             # convert it to list of tensors, dims preserved, just upper dim decomes list
-            return list(axdimm_res)
+            return list(pnm_res)
         for k, sparse_index_group_batch in enumerate(lS_i):
             sparse_offset_group_batch = lS_o[k]
 
@@ -873,15 +873,15 @@ def inference(
     # measurements.
     test_loads = []
     for i, testBatch in enumerate(test_ld):
-        if dlrm.axdimm_tensor_type is not None:
-            axdimm_offsets = []
-            axdimm_indices = torch.as_tensor(torch.cat(testBatch[2]), dtype=torch.int32)
+        if dlrm.pnm_tensor_type is not None:
+            pnm_offsets = []
+            pnm_indices = torch.as_tensor(torch.cat(testBatch[2]), dtype=torch.int32)
             for sparse_offsets_group_batch, sparse_indices_group_batch in zip(testBatch[1], testBatch[2]):
                 spg_list = sparse_offsets_group_batch.tolist()
                 spg_list.append(len(sparse_indices_group_batch))
-                axdimm_offsets.append(spg_list)
-            axdimm_offsets = torch.as_tensor(axdimm_offsets, dtype=torch.int32)
-            testBatch = (testBatch[0], axdimm_offsets, axdimm_indices, testBatch[3])
+                pnm_offsets.append(spg_list)
+            pnm_offsets = torch.as_tensor(pnm_offsets, dtype=torch.int32)
+            testBatch = (testBatch[0], pnm_offsets, pnm_indices, testBatch[3])
         test_loads.append(testBatch)
 
     print("Make {} runs to warmup".format(args.warmup_runs))
@@ -1106,9 +1106,9 @@ def run():
     parser.add_argument("--lr-num-warmup-steps", type=int, default=0)
     parser.add_argument("--lr-decay-start-step", type=int, default=0)
     parser.add_argument("--lr-num-decay-steps", type=int, default=0)
-    parser.add_argument("--use-axdimm", required=False,
-                        choices=[x.name for x in AxdimmTensorType],
-                        help="Axdimm tensor type")
+    parser.add_argument("--use-pnm", required=False,
+                        choices=[x.name for x in PnmTensorType],
+                        help="PNM tensor type")
     parser.add_argument("--weights-path", required=False,
                         help="Embedding weights file path")
     parser.add_argument("--warmup_runs", required=False, type=int,
@@ -1167,13 +1167,13 @@ def run():
         args.test_num_workers = args.num_workers
 
     use_gpu = args.use_gpu and torch.cuda.is_available()
-    use_axdimm = False
-    if args.use_axdimm is not None:
-        if not torch.is_axdimm_available():
-            raise RuntimeError("AXDIMM requested, but PyTorch was compiled without AXDIMM support")
+    use_pnm = False
+    if args.use_pnm is not None:
+        if not torch.is_pnm_available():
+            raise RuntimeError("PNM requested, but PyTorch was compiled without PNM support")
         if not args.inference_only:
-            raise RuntimeError("AXDIMM supports only inference mode. Specify --inference-only to use AXDIMM")
-        use_axdimm = True
+            raise RuntimeError("PNM supports only inference mode. Specify --inference-only to use PNM")
+        use_pnm = True
 
     if not args.debug_mode:
         ext_dist.init_distributed(local_rank=args.local_rank, use_gpu=use_gpu, backend=args.dist_backend)
@@ -1190,8 +1190,8 @@ def run():
         print("Using {} GPU(s)...".format(ngpus))
     else:
         device = torch.device("cpu")
-        if use_axdimm:
-            print("Using AXDIMM...")
+        if use_pnm:
+            print("Using PNM...")
         else:
             print("Using CPU...")
 
@@ -1397,7 +1397,7 @@ def run():
         md_threshold=args.md_threshold,
         weighted_pooling=args.weighted_pooling,
         loss_function=args.loss_function,
-        axdimm_tensor_type=args.use_axdimm,
+        pnm_tensor_type=args.use_pnm,
         weights_path=args.weights_path
     )
 
diff --git a/pnm_utils.py b/pnm_utils.py
new file mode 100644
index 0000000..0a40a46
--- /dev/null
+++ b/pnm_utils.py
@@ -0,0 +1,46 @@
+import torch
+
+import enum
+
+class PnmTensorType(enum.Enum):
+    simple = enum.auto()
+    secure = enum.auto()
+    secure_tagged = enum.auto()
+
+    def __str__(self):
+        return self.name
+
+to_internal_type = {
+    PnmTensorType.simple.name: PnmTensorType.simple.name,
+    PnmTensorType.simple.secure.name: PnmTensorType.simple.secure.name,
+    PnmTensorType.simple.secure_tagged.name: PnmTensorType.simple.secure.name,
+}
+
+class PnmAllocPreference(enum.Enum):
+    auto = 0
+    replicate = 1
+    distribute = 2
+
+    def __str__(self):
+        return self.name
+
+def get_enum(enum_class, name):
+    val = getattr(enum_class, str(name), None)
+    if val is None:
+        raise ValueError("Failed to convert {} to enum of type {}".format(name, enum_class.__name__))
+
+    return val
+
+def create_pnm_tensor_from_file(tensor_type, path, scalar_type, rows, num_tables, sp_size, alloc_pref = PnmAllocPreference.distribute):
+    with_tag = True if "tagged" in str(tensor_type) else False
+    tensor_type = to_internal_type.get(get_enum(PnmTensorType, tensor_type).name)
+    alloc_pref_val = get_enum(PnmAllocPreference, alloc_pref).value
+
+    return torch.ops.pnm.create_tensor(tensor_type, path, scalar_type, rows, num_tables, sp_size, alloc_pref_val, with_tag)
+
+def create_pnm_tensor(tensor_type, weights, alloc_pref = PnmAllocPreference.distribute):
+    with_tag = True if "tagged" in str(tensor_type) else False
+    tensor_type = to_internal_type.get(get_enum(PnmTensorType, tensor_type).name)
+    alloc_pref_val = get_enum(PnmAllocPreference, alloc_pref).value
+
+    return torch.ops.pnm.create_tensor(tensor_type, weights, weights.size(0), weights.size(1), weights.size(2), alloc_pref_val, with_tag)
diff --git a/run_test.sh b/run_test.sh
index a02e5c1..02776ce 100755
--- a/run_test.sh
+++ b/run_test.sh
@@ -13,7 +13,7 @@ num_indices_per_lookup=40
 num_indices_per_lookup_fixed=1
 interaction_op="cat"
 num_batches=100
-axdimm_tensor_type="simple"
+pnm_tensor_type="simple"
 script_run_command="python3"
 script_dir=$(dirname "$0")
 warmup_runs=1
@@ -25,9 +25,9 @@ do
         shift
         weights_path=$1
         ;;
-    "--use-axdimm")
+    "--use-pnm")
         shift
-        axdimm_tensor_type=$1
+        pnm_tensor_type=$1
         ;;
     "--run-command")
         shift
@@ -53,4 +53,4 @@ fi
 $script_run_command $script_dir/dlrm_s_pytorch.py --arch-embedding-size="$embedding_size" --arch-mlp-bot="$arch_mlp_bot" --arch-mlp-top="$arch_mlp_top" \
                             --arch-sparse-feature-size="$sparse_feature_size" --mini-batch-size="$mini_batch_size" --num-indices-per-lookup-fixed="$num_indices_per_lookup_fixed" \
                             --num-indices-per-lookup="$num_indices_per_lookup" --arch-interaction-op="$interaction_op" --num-batches="$num_batches" \
-                            --use-axdimm "$axdimm_tensor_type" --weights-path "$weights_path" --warmup_runs=$warmup_runs --inference-only
+                            --use-pnm "$pnm_tensor_type" --weights-path "$weights_path" --warmup_runs=$warmup_runs --inference-only
diff --git a/test_axdimm_op.py b/test_pnm_op.py
similarity index 85%
rename from test_axdimm_op.py
rename to test_pnm_op.py
index a5f1b55..b7e68a7 100644
--- a/test_axdimm_op.py
+++ b/test_pnm_op.py
@@ -8,10 +8,10 @@ import torch
 import sys
 import torch.autograd.profiler as profiler
 
-from torch.nn import EmbeddingBag, EmbeddingBagAxdimm
+from torch.nn import EmbeddingBag, EmbeddingBagPnm
 from random import randint
 
-from axdimm_utils import *
+from pnm_utils import *
 
 log = logging.getLogger()
 log.setLevel(logging.INFO)
@@ -22,7 +22,7 @@ log.addHandler(handler)
 
 torch.set_printoptions(precision=10)
 
-class AxdimmEmbeddingsTest:
+class PnmEmbeddingsTest:
     tensor_types: set # set of tensor names to test
     tables_num: int # Numer of tables
     table_len: int # Number of features in single table
@@ -106,16 +106,16 @@ class AxdimmEmbeddingsTest:
         self.indices = [self.gen_indices() for x in range(0, self.tables_num)]
         self.offsets = self.gen_offsets()
 
-    def check_results(self, cpu_res, axdimm_res):
+    def check_results(self, cpu_res, pnm_res):
         log.info("Validating results...")
         for table_idx in range(0, self.tables_num):
             for batch_idx in range(0, self.minibatch_size):
-                for cpu_val, axdimm_val in zip(cpu_res[table_idx][batch_idx], axdimm_res[table_idx][batch_idx]):
+                for cpu_val, pnm_val in zip(cpu_res[table_idx][batch_idx], pnm_res[table_idx][batch_idx]):
                     # If at least one element is not close enough - test failed
-                    if not math.isclose(cpu_val.item(), axdimm_val.item(), rel_tol=self.result_tolerance):
-                        log.info("Test failed. CPU and AXDIMM results are not equal")
+                    if not math.isclose(cpu_val.item(), pnm_val.item(), rel_tol=self.result_tolerance):
+                        log.info("Test failed. CPU and PNM results are not equal")
                         log.info("CPU result:\n", cpu_res[table_idx][batch_idx])
-                        log.info("AXDIMM result:\n", axdimm_res[table_idx][batch_idx])
+                        log.info("PNM result:\n", pnm_res[table_idx][batch_idx])
                         return False
         return True
 
@@ -125,29 +125,29 @@ class AxdimmEmbeddingsTest:
         # EmbeddingBag does not support int32, but float32 should work for validation
         cpu_ops = [EmbeddingBag.from_pretrained(torch.tensor(self.tables[i], dtype=torch.float32), mode='sum') for i in range(0, self.tables_num)]
         # CPU version doesn't require last offset, since it knows size of index bag
-        # AXDIMM requires it because indices must be flat 2D list of all indices bags
+        # PNM requires it because indices must be flat 2D list of all indices bags
         # So size of each bag can't be determined from indices bags list
         with profiler.profile(with_stack=True, profile_memory=True) as cpu_prof:
             cpu_res = [cpu_ops[i](torch.tensor(self.indices[i], dtype=torch.int32), torch.tensor(self.offsets[i][:-1], dtype=torch.int32)) for i in range(0, self.tables_num)]
         log.info("CPU profiling results:" + os.linesep + cpu_prof.key_averages(group_by_stack_n=10).table(sort_by='self_cpu_time_total', row_limit=10))
-        log.info("Running EmbeddingBagAxdimm calculations")
+        log.info("Running EmbeddingBagPnm calculations")
         log.info("Preparing data...")
-        axdimm_indices = []
+        pnm_indices = []
         for i_bag in self.indices:
-            axdimm_indices.extend(i_bag)
-        indices_tensor = torch.tensor(axdimm_indices, dtype=torch.int32)
+            pnm_indices.extend(i_bag)
+        indices_tensor = torch.tensor(pnm_indices, dtype=torch.int32)
         offsets_tensor = torch.tensor(self.offsets, dtype=torch.int32)
         for tensor_type in self.tensor_types:
             log.info(f"Loading weights for tensor type: {tensor_type}, scalar type: {self.embedding_scalar_type_name}")
-            op = EmbeddingBagAxdimm(create_axdimm_tensor(tensor_type, self.tables))
-            log.info("Running AXDIMM...")
-            with profiler.profile(with_stack=True, profile_memory=True) as axdimm_prof:
-                axdimm_res = op(indices_tensor, offsets_tensor)
+            op = EmbeddingBagPnm(create_pnm_tensor(tensor_type, self.tables))
+            log.info("Running PNM...")
+            with profiler.profile(with_stack=True, profile_memory=True) as pnm_prof:
+                pnm_res = op(indices_tensor, offsets_tensor)
                 del op
 
-            log.info("AXDIMM %s profiling results:" % tensor_type + os.linesep + axdimm_prof.key_averages(group_by_stack_n=10).table(sort_by='self_cpu_time_total', row_limit=10))
+            log.info("PNM %s profiling results:" % tensor_type + os.linesep + pnm_prof.key_averages(group_by_stack_n=10).table(sort_by='self_cpu_time_total', row_limit=10))
 
-            if self.check_results(cpu_res, axdimm_res):
+            if self.check_results(cpu_res, pnm_res):
                 log.info("Test completed successfully")
             else:
                 log.info("Test failed")
@@ -188,7 +188,7 @@ def validate_args(args):
     
 if __name__ == "__main__":
     parser = argparse.ArgumentParser(
-        description="Test Axdimm embedding sum operator"
+        description="Test Pnm embedding sum operator"
     )
     parser.add_argument("--sparse-feature-size", type=int, default=16, help="Number of elements in feature, must be >= 16")
     parser.add_argument("--minibatch-size", type=int, default=256, help="Numer of lookups per table, must be > 0")
@@ -200,14 +200,14 @@ if __name__ == "__main__":
     parser.add_argument("--randomize-lookups-len", default=False, action="store_true", help="Pick random lookup length for each table batch")
     parser.add_argument("--fixed-lookups-len", type=int, default=40, help="Fixed length for not randomized lookups, must be > 0")
     parser.add_argument("--result-tolerance", type=float, default=0.001, help="Result precision tolerance, must be > 0")
-    parser.add_argument("--tensor-types", nargs='+', default=[x.name for x in AxdimmTensorType], choices=[x.name for x in AxdimmTensorType],
-                        help="Axdimm tensor types")
+    parser.add_argument("--tensor-types", nargs='+', default=[x.name for x in PnmTensorType], choices=[x.name for x in PnmTensorType],
+                        help="Pnm tensor types")
     args = parser.parse_args()
     if not validate_args(args):
         log.info("Arguments validation failed")
         sys.exit(-1)
 
     for embedding_scalar_type_name in ["int32", "float32"]:
-        test = AxdimmEmbeddingsTest(args, embedding_scalar_type_name)
+        test = PnmEmbeddingsTest(args, embedding_scalar_type_name)
         if not test.run_test():
             sys.exit(-1)
-- 
2.34.1

